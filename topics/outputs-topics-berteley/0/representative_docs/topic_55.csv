doc
unsupervise architecture representation learning help neural architecture search exist neural architecture search nas method encode neural architecture discrete encoding scale adopt supervise learning base method jointly learn architecture representation optimize architecture search representation incur search bias despite widespread use architecture representation learn nas poorly understand observe structural property neural architecture hard preserve latent space architecture representation learning search couple result effective search performance work find empirically pre training architecture representation neural architecture accuracy label considerably improve downstream architecture search efficiency explain observation visualize unsupervised architecture representation learn well encourage neural architecture similar connection operator cluster help map neural architecture similar performance region latent space make transition architecture latent space relatively smooth considerably benefit diverse downstream search strategy
search space adaptation differentiable neural architecture search image classification deep neural network achieve unprecedented performance task neural architecture search nas research field design neural network architecture automated process actively underway recently differentiable nas great impact reduce search cost level train single network search space define candidate architecture search directly affect performance final architecture paper propose adaptation scheme search space introduce search scope effectiveness propose method demonstrate proxylessna image classification task furthermore visualize trajectory architecture parameter update provide insight improve architecture search
dass differentiable architecture search sparse neural network deployment deep neural network dnn edge device hinder substantial gap performance requirement available processing power recent research significant stride develop pruning method build sparse network reduce compute overhead dnn remain considerable accuracy loss especially high pruning ratio find architecture design dense network differentiable architecture search method ineffective prune mechanism apply main reason current method support sparse architecture search space use search objective dense network pay attention sparsity paper propose new method search sparsity friendly neural architecture add new sparse operation search space modify search objective propose novel parametric sparseconv sparselinear operation order expand search space include sparse operation particular operation flexible search space sparse parametric version linear convolution operation propose search objective let train architecture base sparsity search space operation quantitative analysis demonstrate search architecture outperform stateof art sparse network imagenet dataset term performance hardware effectiveness dass increase accuracy sparse version mobilenet improvement fast inference time
