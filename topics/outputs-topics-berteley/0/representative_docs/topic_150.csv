doc
"approximation of nonlinear functionals using deep relu networks. in recent years, functional neural networks have been proposed and studied in order to approximate nonlinear continuous functionals defined on $l^p([-1, 1]^s)$ for integers $s\ge1$ and $1\le p<\infty$. however, their theoretical properties are largely unknown beyond universality of approximation or the existing analysis does not apply to the rectified linear unit (relu) activation function. to fill in this void, we investigate here the approximation power of functional deep neural networks associated with the relu activation function by constructing a continuous piecewise linear interpolation under a simple triangulation. in addition, we establish rates of approximation of the proposed functional deep relu networks under mild regularity conditions. finally, our study may also shed some light on the understanding of functional data learning algorithms."
"learning relu networks on linearly separable data: algorithm, optimality, and generalization. neural networks with rectified linear unit (relu) activation functions (a.k.a. relu networks) have achieved great empirical success in various domains. nonetheless, existing results for learning relu networks either pose assumptions on the underlying data distribution being e.g. gaussian, or require the network size and/or training size to be sufficiently large. in this context, the problem of learning a two-layer relu network is approached in a binary classification setting, where the data are linearly separable and a hinge loss criterion is adopted. leveraging the power of random noise perturbation, this paper presents a novel stochastic gradient descent (sgd) algorithm, which can \emph{provably} train any single-hidden-layer relu network to attain global optimality, despite the presence of infinitely many bad local minima, maxima, and saddle points in general. this result is the first of its kind, requiring no assumptions on the data distribution, training/network size, or initialization. convergence of the resultant iterative algorithm to a global minimum is analyzed by establishing both an upper bound and a lower bound on the number of non-zero updates to be performed. moreover, generalization guarantees are developed for relu networks trained with the novel sgd leveraging classic compression bounds. these guarantees highlight a key difference (at least in the worst case) between reliably learning a relu network as well as a leaky relu network in terms of sample complexity. numerical tests using both synthetic data and real images validate the effectiveness of the algorithm and the practical merits of the theory."
"complexity of training relu neural network. in this paper, we explore some basic questions on the complexity of training neural networks with relu activation function. we show that it is np-hard to train a two-hidden layer feedforward relu neural network. if dimension of the input data and the network topology is fixed, then we show that there exists a polynomial time algorithm for the same training problem. we also show that if sufficient over-parameterization is provided in the first hidden layer of relu neural network, then there is a polynomial time algorithm which finds weights such that output of the over-parameterized relu neural network matches with the output of the given data."
