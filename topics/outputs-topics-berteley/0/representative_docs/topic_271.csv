doc
"the impact of model size on catastrophic forgetting in online continual learning. this study investigates the impact of model size on online continual learning performance, with a focus on catastrophic forgetting. employing resnet architectures of varying sizes, the research examines how network depth and width affect model performance in class-incremental learning using the splitcifar-10 dataset. key findings reveal that larger models do not guarantee better continual learning performance; in fact, they often struggle more in adapting to new tasks, particularly in online settings. these results challenge the notion that larger models inherently mitigate catastrophic forgetting, highlighting the nuanced relationship between model size and continual learning efficacy. this study contributes to a deeper understanding of model scalability and its practical implications in continual learning scenarios."
"on the convergence of continual learning with adaptive methods. one of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma. however, the convergence of continual learning for each sequential task is less studied so far. in this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks. we propose an adaptive method for nonconvex continual learning (nccl), which adjusts step sizes of both previous and current tasks with the gradients. the proposed method can achieve the same convergence rate as the sgd method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration. further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks."
"how efficient are today's continual learning algorithms?. supervised continual learning involves updating a deep neural network (dnn) from an ever-growing stream of labeled data. while most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. some methods even require more compute than training from scratch! we argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. there is more to continual learning than mitigating catastrophic forgetting."
