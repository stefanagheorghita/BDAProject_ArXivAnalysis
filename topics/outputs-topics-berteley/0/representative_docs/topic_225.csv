doc
"optimized local updates in federated learning via reinforcement learning. federated learning (fl) is a distributed framework for collaborative model training over large-scale distributed data, enabling higher performance while maintaining client data privacy. however, the nature of model aggregation at the centralized server can result in a performance drop in the presence of non-iid data across different clients. we remark that training a client locally on more data than necessary does not benefit the overall performance of all clients. in this paper, we devise a novel framework that leverages a deep reinforcement learning (drl) agent to select an optimized amount of data necessary to train a client model without oversharing information with the server. starting without awareness of the client's performance, the drl agent utilizes the change in training loss as a reward signal and learns to optimize the amount of training data necessary for improving the client's performance. specifically, after each aggregation round, the drl algorithm considers the local performance as the current state and outputs the optimized weights for each class, in the training data, to be used during the next round of local training. in doing so, the agent learns a policy that creates an optimized partition of the local training dataset during the fl rounds. after fl, the client utilizes the entire local training dataset to further enhance its performance on its own data distribution, mitigating the non-iid effects of aggregation. through extensive experiments, we demonstrate that training fl clients through our algorithm results in superior performance on multiple benchmark datasets and fl frameworks. our code is available at https://github.com/amuraddd/optimized_client_training.git."
"personalized interpretation on federated learning: a virtual concepts approach. tackling non-iid data is an open challenge in federated learning research. existing fl methods, including robust fl and personalized fl, are designed to improve model performance without consideration of interpreting non-iid across clients. this paper aims to design a novel fl method to robust and interpret the non-iid data across clients. specifically, we interpret each client's dataset as a mixture of conceptual vectors that each one represents an interpretable concept to end-users. these conceptual vectors could be pre-defined or refined in a human-in-the-loop process or be learnt via the optimization procedure of the federated learning system. in addition to the interpretability, the clarity of client-specific personalization could also be applied to enhance the robustness of the training process on fl system. the effectiveness of the proposed method have been validated on benchmark datasets."
"fl games: a federated learning framework for distribution shifts. federated learning aims to train predictive models for data that is distributed across clients, under the orchestration of a server. however, participating clients typically each hold data from a different distribution, whereby predictive models with strong in-distribution generalization can fail catastrophically on unseen domains. in this work, we argue that in order to generalize better across non-i.i.d. clients, it is imperative to only learn correlations that are stable and invariant across domains. we propose fl games, a game-theoretic framework for federated learning for learning causal features that are invariant across clients. while training to achieve the nash equilibrium, the traditional best response strategy suffers from high-frequency oscillations. we demonstrate that fl games effectively resolves this challenge and exhibits smooth performance curves. further, fl games scales well in the number of clients, requires significantly fewer communication rounds, and is agnostic to device heterogeneity. through empirical evaluation, we demonstrate that fl games achieves high out-of-distribution performance on various benchmarks."
