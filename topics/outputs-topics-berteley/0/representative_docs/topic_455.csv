doc
"privacy risks of securing machine learning models against adversarial examples. the arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. however, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. it is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. in this paper, we take a step towards resolving this limitation by combining the two domains. in particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). membership inference attacks determine whether or not an individual data record has been part of a model's training set. the accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. however, this objective is optimized on training data. thus, individual data records in the training set have a significant influence on robust models. this makes the models more vulnerable to inference attacks. to perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. we also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks."
"a reliable cryptographic framework for empirical machine unlearning evaluation. machine unlearning updates machine learning models to remove information from specific training samples, complying with data protection regulations that allow individuals to request the removal of their personal data. despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. in this work, we focus on membership inference attack (mia) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics lacking theoretical understanding and reliability. specifically, by modeling the proposed evaluation process as a \emph{cryptographic game} between unlearning algorithms and mia adversaries, the naturally induced evaluation metric measures the data removal efficacy of unlearning algorithms and enjoys provable guarantees that existing evaluation metrics fail to satisfy. furthermore, we propose a practical and efficient approximation of the induced evaluation metric and demonstrate its effectiveness through both theoretical analysis and empirical experiments. overall, this work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques."
"layer attack unlearning: fast and accurate machine unlearning via layer level attack and knowledge distillation. recently, serious concerns have been raised about the privacy issues related to training datasets in machine learning algorithms when including personal data. various regulations in different countries, including the gdpr grant individuals to have personal data erased, known as 'the right to be forgotten' or 'the right to erasure'. however, there has been less research on effectively and practically deleting the requested personal data from the training set while not jeopardizing the overall machine learning performance. in this work, we propose a fast and novel machine unlearning paradigm at the layer level called layer attack unlearning, which is highly accurate and fast compared to existing machine unlearning algorithms. we introduce the partial-pgd algorithm to locate the samples to forget efficiently. in addition, we only use the last layer of the model inspired by the forward-forward algorithm for unlearning process. lastly, we use knowledge distillation (kd) to reliably learn the decision boundaries from the teacher using soft label information to improve accuracy performance. we conducted extensive experiments with sota machine unlearning models and demonstrated the effectiveness of our approach for accuracy and end-to-end unlearning performance."
