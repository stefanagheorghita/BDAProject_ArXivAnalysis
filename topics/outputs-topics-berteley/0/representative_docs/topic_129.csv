doc
"joint pruning and channel-wise mixed-precision quantization for efficient deep neural networks. the resource requirements of deep neural networks (dnns) pose significant challenges to their deployment on edge devices. common approaches to address this issue are pruning and mixed-precision quantization, which lead to latency and memory occupation improvements. these optimization techniques are usually applied independently. we propose a novel methodology to apply them jointly via a lightweight gradient-based search, and in a hardware-aware manner, greatly reducing the time required to generate pareto-optimal dnns in terms of accuracy versus cost (i.e., latency or memory). we test our approach on three edge-relevant benchmarks, namely cifar-10, google speech commands, and tiny imagenet. when targeting the optimization of the memory footprint, we are able to achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the baseline networks with all weights quantized at 8 and 2-bit, respectively. our method surpasses a previous state-of-the-art approach with up to 56.17% size reduction at iso-accuracy. with respect to the sequential application of state-of-the-art pruning and mixed-precision optimizations, we obtain comparable or superior results, but with a significantly lowered training time. in addition, we show how well-tailored cost models can improve the cost versus accuracy trade-offs when targeting specific hardware for deployment."
"scaling neural network performance through customized hardware architectures on reconfigurable logic. convolutional neural networks have dramatically improved in recent years, surpassing human accuracy on certain problems and performance exceeding that of traditional computer vision algorithms. while the compute pattern in itself is relatively simple, significant compute and memory challenges remain as cnns may contain millions of floating-point parameters and require billions of floating-point operations to process a single image. these computational requirements, combined with storage footprints that exceed typical cache sizes, pose a significant performance and power challenge for modern compute architectures. one of the promising opportunities to scale performance and power efficiency is leveraging reduced precision representations for all activations and weights as this allows to scale compute capabilities, reduce weight and feature map buffering requirements as well as energy consumption. while a small reduction in accuracy is encountered, these quantized neural networks have been shown to achieve state-of-the-art accuracy on standard benchmark datasets, such as mnist, cifar-10, svhn and even imagenet, and thus provide highly attractive design trade-offs. current research has focused mainly on the implementation of extreme variants with full binarization of weights and or activations, as well typically smaller input images. within this paper, we investigate the scalability of dataflow architectures with respect to supporting various precisions for both weights and activations, larger image dimensions, and increasing numbers of feature map channels. key contributions are a formalized approach to understanding the scalability of the existing hardware architecture with cost models and a performance prediction as a function of the target device size. we provide validating experimental results for an imagenet classification on a server-class platform, namely the aws f1 node."
"least squares binary quantization of neural networks. quantizing weights and activations of deep neural networks results in significant improvement in inference efficiency at the cost of lower accuracy. a source of the accuracy gap between full precision and quantized models is the quantization error. in this work, we focus on the binary quantization, in which values are mapped to -1 and 1. we provide a unified framework to analyze different scaling strategies. inspired by the pareto-optimality of 2-bits versus 1-bit quantization, we introduce a novel 2-bits quantization with provably least squares error. our quantization algorithms can be implemented efficiently on the hardware using bitwise operations. we present proofs to show that our proposed methods are optimal, and also provide empirical error analysis. we conduct experiments on the imagenet dataset and show a reduced accuracy gap when using the proposed least squares quantization algorithms."
