doc
"geodesic calculus on latent spaces. latent manifolds of autoencoders provide low-dimensional representations of data, which can be studied from a geometric perspective. we propose to describe these latent manifolds as implicit submanifolds of some ambient latent space. based on this, we develop tools for a discrete riemannian calculus approximating classical geometric operators. these tools are robust against inaccuracies of the implicit representation often occurring in practical examples. to obtain a suitable implicit representation, we propose to learn an approximate projection onto the latent manifold by minimizing a denoising objective. this approach is independent of the underlying autoencoder and supports the use of different riemannian geometries on the latent manifolds. the framework in particular enables the computation of geodesic paths connecting given end points and shooting geodesics via the riemannian exponential maps on latent manifolds. we evaluate our approach on various autoencoders trained on synthetic and real data."
"mixed-curvature variational autoencoders. euclidean geometry has historically been the typical ""workhorse"" for machine learning applications due to its power and simplicity. however, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. consequently, generative models like variational autoencoders (vaes) have been successfully generalized to elliptical and hyperbolic latent spaces. while these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic vae, there exists no generic approach unifying and leveraging all three models. we develop a mixed-curvature variational autoencoder, an efficient way to train a vae whose latent space is a product of constant curvature riemannian manifolds, where the per-component curvature is fixed or learnable. this generalizes the euclidean vae to curved latent spaces and recovers it when curvatures of all latent space components go to 0."
"encoded prior sliced wasserstein autoencoder for learning latent manifold representations. while variational autoencoders have been successful in several tasks, the use of conventional priors are limited in their ability to encode the underlying structure of input data. we introduce an encoded prior sliced wasserstein autoencoder wherein an additional prior-encoder network learns an embedding of the data manifold which preserves topological and geometric properties of the data, thus improving the structure of latent space. the autoencoder and prior-encoder networks are iteratively trained using the sliced wasserstein distance. the effectiveness of the learned manifold encoding is explored by traversing latent space through interpolations along geodesics which generate samples that lie on the data manifold and hence are more realistic compared to euclidean interpolation. to this end, we introduce a graph-based algorithm for exploring the data manifold and interpolating along network-geodesics in latent space by maximizing the density of samples along the path while minimizing total energy. we use the 3d-spiral data to show that the prior encodes the geometry underlying the data unlike conventional autoencoders, and to demonstrate the exploration of the embedded data manifold through the network algorithm. we apply our framework to benchmarked image datasets to demonstrate the advantages of learning data representations in outlier generation, latent structure, and geodesic interpolation."
