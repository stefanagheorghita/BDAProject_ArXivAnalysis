doc
"regret balancing for bandit and rl model selection. we consider model selection in stochastic bandit and reinforcement learning problems. given a set of base learning algorithms, an effective model selection strategy adapts to the best learning algorithm in an online fashion. we show that by estimating the regret of each algorithm and playing the algorithms such that all empirical regrets are ensured to be of the same order, the overall regret balancing strategy achieves a regret that is close to the regret of the optimal base algorithm. our strategy requires an upper bound on the optimal base regret as input, and the performance of the strategy depends on the tightness of the upper bound. we show that having this prior knowledge is necessary in order to achieve a near-optimal regret. further, we show that any near-optimal model selection strategy implicitly performs a form of regret balancing."
"adaptive regret for bandits made possible: two queries suffice. fast changing states or volatile environments pose a significant challenge to online optimization, which needs to perform rapid adaptation under limited observation. in this paper, we give query and regret optimal bandit algorithms under the strict notion of strongly adaptive regret, which measures the maximum regret over any contiguous interval $i$. due to its worst-case nature, there is an almost-linear $\omega(|i|^{1-\epsilon})$ regret lower bound, when only one query per round is allowed [daniely el al, icml 2015]. surprisingly, with just two queries per round, we give strongly adaptive bandit learner (stabl) that achieves $\tilde{o}(\sqrt{n|i|})$ adaptive regret for multi-armed bandits with $n$ arms. the bound is tight and cannot be improved in general. our algorithm leverages a multiplicative update scheme of varying stepsizes and a carefully chosen observation distribution to control the variance. furthermore, we extend our results and provide optimal algorithms in the bandit convex optimization setting. finally, we empirically demonstrate the superior performance of our algorithms under volatile environments and for downstream tasks, such as algorithm selection for hyperparameter optimization."
memory-constrained no-regret learning in adversarial bandits. an adversarial bandit problem with memory constraints is studied where only the statistics of a subset of arms can be stored. a hierarchical learning policy that requires only a sublinear order of memory space in terms of the number of arms is developed. its sublinear regret orders with respect to the time horizon are established for both weak regret and shifting regret. this work appears to be the first on memory-constrained bandit problems under the adversarial setting.
