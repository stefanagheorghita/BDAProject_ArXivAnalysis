doc
pre training universal language representation despite develop cut edge representation learning language language representation model usually focus specific level linguistic unit work introduce universal language representation learning embedding different level linguistic unit text diverse length uniform vector space propose training objective misad utilize meaningful n gram extract large unlabeled corpus simple effective algorithm pre train language model empirically verify design pre training scheme effectively yield universal language representation bring great convenience handle multiple layer linguistic object unified way especially model achieve high accuracy analogy task different language level significantly improve performance downstream task glue benchmark question answer dataset
transferable modeling strategy low resource llm task prompt alignment base approach paper address limit transfer adaptation capability large language model low resource language scenario propose unified framework combine knowledge transfer module parameter efficient fine tuning strategy method introduce knowledge alignment loss soft prompt tuning guide model effectively absorb structural feature target language task minimal annotation enhance generalization performance training stability framework include lightweight adaptation module reduce computational cost training integrate freeze strategy prompt injection preserve model original knowledge enable quick adaptation new task study conduct stability analysis experiment synthetic pseudo data transfer experiment systematically evaluate method applicability robustness different low resource task experimental result compare exist multilingual pre train model mainstream transfer method propose approach achieve high performance stability cross lingual task mlqa xquad paws demonstrate particularly strong advantage extremely data scarce condition propose method offer strong generality scalability enhance task specific adaptability preserve general capability large language model make suited complex semantic modeling multilingual processing task
reasoning transfer extremely low resource endangered language bridge language sample efficient language understanding recent advance enable large language model llm tackle reasoning task generate chain thought cot rationale gain largely apply high resource language leave low resource language work investigate cot technique extremely low resource scenario previous prompting model editing fine tuning approach introduce english pivot cot training leverage insight llm internally operate latent space align dominant language give input low resource language perform supervise fine tuning generate cot english output final response target language mathematical reasoning benchmark approach outperform baseline improvement low resource scenario analysis additional experiment include mixed language cot stage training explicitly separate language understanding reasoning enhance cross lingual reasoning ability facilitate future work release benchmark mathematical task irish extremely low resource endangered language result resource highlight practical pathway multilingual reasoning extensive retraining extremely low resource language despite datum scarcity
