doc
"dat++: spatially dynamic vision transformer with deformable attention. transformers have shown superior performance on various vision tasks. their large receptive field endows transformer models with higher representation power than their cnn counterparts. nevertheless, simply enlarging the receptive field also raises several concerns. on the one hand, using dense attention in vit leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. on the other hand, the handcrafted attention adopted in pvt or swin transformer is data agnostic and may limit the ability to model long-range relations. to solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. this flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. on this basis, we present deformable attention transformer (dat), a general vision backbone efficient and effective for visual recognition. we further build an enhanced version dat++. extensive experiments show that our dat++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% imagenet accuracy, 54.5 and 47.0 ms-coco instance segmentation map, and 51.5 ade20k semantic segmentation miou."
"visual transformer for object detection. convolutional neural networks (cnn) have been the first choice of paradigm in many computer vision applications. the convolution operation however has a significant weakness which is it only operates on a local neighborhood of pixels, thus it misses global information of the surrounding neighbors. transformers, or self-attention networks to be more specific, on the other hand, have emerged as a recent advance to capture long range interactions of the input, but they have mostly been applied to sequence modeling tasks such as neural machine translation, image captioning and other natural language processing tasks. transformers has been applied to natural language related tasks and achieved promising results. however, its applications in visual related tasks are far from being satisfying. taking into consideration of both the weaknesses of convolutional neural networks and those of the transformers, in this paper, we consider the use of self-attention for discriminative visual tasks, object detection, as an alternative to convolutions. in this paper, we propose our model: dettransnet. extensive experiments show that our model leads to consistent improvements in object detection on coco across many different models and scales, including resnets, while keeping the number of parameters similar. in particular, our method achieves a 1.2% average precision improvement on coco object detection task over other baseline models."
"focal self-attention for local-global interactions in vision transformers. recently, vision transformer and its variants have shown great promise on various computer vision tasks. the ability of capturing short- and long-range visual dependencies through self-attention is arguably the main source for the success. but it also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). in this paper, we present focal self-attention, a new mechanism that incorporates both fine-grained local and coarse-grained global interactions. using this new mechanism, each token attends the closest surrounding tokens at fine granularity but the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively. with focal self-attention, we propose a new variant of vision transformer models, called focal transformer, which achieves superior performance over the state-of-the-art vision transformers on a range of public image classification and object detection benchmarks. in particular, our focal transformer models with a moderate size of 51.1m and a larger size of 89.8m achieve 83.5 and 83.8 top-1 accuracy, respectively, on imagenet classification at 224x224 resolution. using focal transformers as the backbones, we obtain consistent and substantial improvements over the current state-of-the-art swin transformers for 6 different object detection methods trained with standard 1x and 3x schedules. our largest focal transformer yields 58.7/58.9 box maps and 50.9/51.3 mask maps on coco mini-val/test-dev, and 55.4 miou on ade20k for semantic segmentation, creating new sota on three of the most challenging computer vision tasks."
