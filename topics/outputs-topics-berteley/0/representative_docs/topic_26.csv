doc
sqsgd locally private communication efficient federate learning federated learning fl technique train machine learning model decentralized datum source study fl local notion privacy constraint provide strong protection sensitive datum disclosure obfuscate datum leave client identify major concern design practical privacy preserve fl algorithm communication efficiency high dimensional compatibility develop gradient base learn algorithm call selective quantize stochastic gradient descent address concern proposed algorithm base novel privacy preserve quantization scheme use constant number bit dimension client improve base algorithm way apply gradient subsampling strategy simultaneously offer well training performance small communication cost fix privacy budget secondly utilize randomized rotation preprocesse step reduce quantization error thirdly adaptive gradient norm upper bind shrinkage strategy adopt improve accuracy stabilize training finally practicality propose framework demonstrate benchmark dataset experiment result sqsgd successfully learn large model like lenet resnet local privacy constraint addition fix privacy communication level performance sqsgd significantly dominate baseline algorithm
optimize local update federate learning reinforcement learning federated learning fl distribute framework collaborative model training large scale distribute datum enable high performance maintain client datum privacy nature model aggregation centralized server result performance drop presence non iid datum different client remark train client locally datum necessary benefit overall performance client paper devise novel framework leverage deep reinforcement learning drl agent select optimize datum necessary train client model overshare information server start awareness client performance drl agent utilize change training loss reward signal learn optimize training datum necessary improve client performance specifically aggregation round drl algorithm consider local performance current state output optimize weight class training datum round local training agent learn policy create optimize partition local training dataset fl round fl client utilize entire local training dataset enhance performance datum distribution mitigate non iid effect aggregation extensive experiment demonstrate train fl client algorithm result superior performance multiple benchmark dataset fl framework code available
privacy preserve self teach federate learning heterogeneous datum application scenario train machine learning model multiple participant federate learning fl propose enable joint training deep learning model local datum party reveal datum type fl method vertical fl category handle data source d space different feature space exist vertical fl method suffer limitation restrictive neural network structure slow training speed lack ability advantage datum unmatched id work propose fl method call self teach federate learning address aforementioned issue use unsupervised feature extraction technique distribute supervise deep learning task method latent variable transmit party model training privacy preserve store datum parameter activation weight bias locally extensive experiment perform evaluate demonstrate validity efficiency propose method
