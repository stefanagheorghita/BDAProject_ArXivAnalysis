doc
"anisotropic neural representation learning for high-quality neural rendering. neural radiance fields (nerfs) have achieved impressive view synthesis results by learning an implicit volumetric representation from multi-view images. to project the implicit representation into an image, nerf employs volume rendering that approximates the continuous integrals of rays as an accumulation of the colors and densities of the sampled points. although this approximation enables efficient rendering, it ignores the direction information in point intervals, resulting in ambiguous features and limited reconstruction quality. in this paper, we propose an anisotropic neural representation learning method that utilizes learnable view-dependent features to improve scene representation and reconstruction. we model the volumetric function as spherical harmonic (sh)-guided anisotropic features, parameterized by multilayer perceptrons, facilitating ambiguity elimination while preserving the rendering efficiency. to achieve robust scene reconstruction without anisotropy overfitting, we regularize the energy of the anisotropic features during training. our method is flexiable and can be plugged into nerf-based frameworks. extensive experiments show that the proposed representation can boost the rendering quality of various nerfs and achieve state-of-the-art rendering performance on both synthetic and real-world scenes."
"real-time high-resolution view synthesis of complex scenes with explicit 3d visibility reasoning. rendering photo-realistic novel-view images of complex scenes has been a long-standing challenge in computer graphics. in recent years, great research progress has been made on enhancing rendering quality and accelerating rendering speed in the realm of view synthesis. however, when rendering complex dynamic scenes with sparse views, the rendering quality remains limited due to occlusion problems. besides, for rendering high-resolution images on dynamic scenes, the rendering speed is still far from real-time. in this work, we propose a generalizable view synthesis method that can render high-resolution novel-view images of complex static and dynamic scenes in real-time from sparse views. to address the occlusion problems arising from the sparsity of input views and the complexity of captured scenes, we introduce an explicit 3d visibility reasoning approach that can efficiently estimate the visibility of sampled 3d points to the input views. the proposed visibility reasoning approach is fully differentiable and can gracefully fit inside the volume rendering pipeline, allowing us to train our networks with only multi-view images as supervision while refining geometry and texture simultaneously. besides, each module in our pipeline is carefully designed to bypass the time-consuming mlp querying process and enhance the rendering quality of high-resolution images, enabling us to render high-resolution novel-view images in real-time.experimental results show that our method outperforms previous view synthesis methods in both rendering quality and speed, particularly when dealing with complex dynamic scenes with sparse views."
"pointgs: point attention-aware sparse view synthesis with gaussian splatting. 3d gaussian splatting (3dgs) is an innovative rendering technique that surpasses the neural radiance field (nerf) in both rendering speed and visual quality by leveraging an explicit 3d scene representation. existing 3dgs approaches require a large number of calibrated views to generate a consistent and complete scene representation. when input views are limited, 3dgs tends to overfit the training views, leading to noticeable degradation in rendering quality. to address this limitation, we propose a point-wise feature-aware gaussian splatting framework that enables real-time, high-quality rendering from sparse training views. specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for gaussian initialization. we then encode the colour attributes of each 3d gaussian by sampling and aggregating multiscale 2d appearance features from sparse inputs. to enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each gaussian point to interact with its nearest neighbors. these enriched features are subsequently decoded into gaussian parameters through two lightweight multi-layer perceptrons (mlps) for final rendering. extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms nerf-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3dgs methods."
