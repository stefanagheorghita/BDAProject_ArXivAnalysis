doc
"optimal counterfactual explanations in tree ensembles. counterfactual explanations are usually generated through heuristics that are sensitive to the search's initial conditions. the absence of guarantees of performance and robustness hinders trustworthiness. in this paper, we take a disciplined approach towards counterfactual explanations for tree ensembles. we advocate for a model-based search aiming at ""optimal"" explanations and propose efficient mixed-integer programming approaches. we show that isolation forests can be modeled within our framework to focus the search on plausible explanations with a low outlier score. we provide comprehensive coverage of additional constraints that model important objectives, heterogeneous data types, structural constraints on the feature space, along with resource and actionability restrictions. our experimental analyses demonstrate that the proposed search approach requires a computational effort that is orders of magnitude smaller than previous mathematical programming algorithms. it scales up to large data sets and tree ensembles, where it provides, within seconds, systematic explanations grounded on well-defined models solved to optimality."
"when can you trust your explanations? a robustness analysis on feature importances. recent legislative regulations have underlined the need for accountable and transparent artificial intelligence systems and have contributed to a growing interest in the explainable artificial intelligence (xai) field. nonetheless, the lack of standardized criteria to validate explanation methodologies remains a major obstacle to developing trustworthy systems. we address a crucial yet often overlooked aspect of xai, the robustness of explanations, which plays a central role in ensuring trust in both the system and the provided explanation. to this end, we propose a novel approach to analyse the robustness of neural network explanations to non-adversarial perturbations, leveraging the manifold hypothesis to produce new perturbed datapoints that resemble the observed data distribution. we additionally present an ensemble method to aggregate various explanations, showing how merging explanations can be beneficial for both understanding the model's decision and evaluating the robustness. the aim of our work is to provide practitioners with a framework for evaluating the trustworthiness of model explanations. experimental results on feature importances derived from neural networks applied to tabular datasets highlight the importance of robust explanations in practical applications."
"counterfactual explanations for oblique decision trees: exact, efficient algorithms. we consider counterfactual explanations, the problem of minimally adjusting features in a source input instance so that it is classified as a target class under a given classifier. this has become a topic of recent interest as a way to query a trained model and suggest possible actions to overturn its decision. mathematically, the problem is formally equivalent to that of finding adversarial examples, which also has attracted significant attention recently. most work on either counterfactual explanations or adversarial examples has focused on differentiable classifiers, such as neural nets. we focus on classification trees, both axis-aligned and oblique (having hyperplane splits). although here the counterfactual optimization problem is nonconvex and nondifferentiable, we show that an exact solution can be computed very efficiently, even with high-dimensional feature vectors and with both continuous and categorical features, and demonstrate it in different datasets and settings. the results are particularly relevant for finance, medicine or legal applications, where interpretability and counterfactual explanations are particularly important."
