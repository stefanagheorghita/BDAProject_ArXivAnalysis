doc
"emobed: strengthening monomodal emotion recognition via training with crossmodal emotion embeddings. despite remarkable advances in emotion recognition, they are severely restrained from either the essentially limited property of the employed single modality, or the synchronous presence of all involved multiple modalities. motivated by this, we propose a novel crossmodal emotion embedding framework called emobed, which aims to leverage the knowledge from other auxiliary modalities to improve the performance of an emotion recognition system at hand. the framework generally includes two main learning components, i. e., joint multimodal training and crossmodal training. both of them tend to explore the underlying semantic emotion information but with a shared recognition network or with a shared emotion embedding space, respectively. in doing this, the enhanced system trained with this approach can efficiently make use of the complementary information from other modalities. nevertheless, the presence of these auxiliary modalities is not demanded during inference. to empirically investigate the effectiveness and robustness of the proposed framework, we perform extensive experiments on the two benchmark databases recola and omg-emotion for the tasks of dimensional emotion regression and categorical emotion classification, respectively. the obtained results show that the proposed framework significantly outperforms related baselines in monomodal inference, and are also competitive or superior to the recently reported systems, which emphasises the importance of the proposed crossmodal learning for emotion recognition."
"modality influence in multimodal machine learning. multimodal machine learning has emerged as a prominent research direction across various applications such as sentiment analysis, emotion recognition, machine translation, hate speech recognition, and movie genre classification. this approach has shown promising results by utilizing modern deep learning architectures. despite the achievements made, challenges remain in data representation, alignment techniques, reasoning, generation, and quantification within multimodal learning. additionally, assumptions about the dominant role of textual modality in decision-making have been made. however, limited investigations have been conducted on the influence of different modalities in multimodal machine learning systems. this paper aims to address this gap by studying the impact of each modality on multimodal learning tasks. the research focuses on verifying presumptions and gaining insights into the usage of different modalities. the main contribution of this work is the proposal of a methodology to determine the effect of each modality on several multimodal machine learning models and datasets from various tasks. specifically, the study examines multimodal sentiment analysis, multimodal emotion recognition, multimodal hate speech recognition, and multimodal disease detection. the study objectives include training sota multimodal machine learning models with masked modalities to evaluate their impact on performance. furthermore, the research aims to identify the most influential modality or set of modalities for each task and draw conclusions for diverse multimodal classification tasks. by undertaking these investigations, this research contributes to a better understanding of the role of individual modalities in multi-modal learning and provides valuable insights for future advancements in this field."
"faf: a novel multimodal emotion recognition approach integrating face, body and text. multimodal emotion analysis performed better in emotion recognition depending on more comprehensive emotional clues and multimodal emotion dataset. in this paper, we developed a large multimodal emotion dataset, named ""hed"" dataset, to facilitate the emotion recognition task, and accordingly propose a multimodal emotion recognition method. to promote recognition accuracy, ""feature after feature"" framework was used to explore crucial emotional information from the aligned face, body and text samples. we employ various benchmarks to evaluate the ""hed"" dataset and compare the performance with our method. the results show that the five classification accuracy of the proposed multimodal fusion method is about 83.75%, and the performance is improved by 1.83%, 9.38%, and 21.62% respectively compared with that of individual modalities. the complementarity between each channel is effectively used to improve the performance of emotion recognition. we had also established a multimodal online emotion prediction platform, aiming to provide free emotion prediction to more users."
