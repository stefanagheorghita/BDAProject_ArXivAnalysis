doc
"fact-and-reflection (far) improves confidence calibration of large language models. for a llm to be trustworthy, its confidence level should be well-calibrated with its actual performance. while it is now common sense that llm performances are greatly impacted by prompts, the confidence calibration in prompting llms has yet to be thoroughly explored. in this paper, we explore how different prompting strategies influence llm confidence calibration and how it could be improved. we conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected llm calibration, they also trigger llms to be over-confident when responding to some instances. inspired by human cognition, we propose fact-and-reflection (far) prompting, which improves the llm calibration in two steps. first, far elicits the known ""facts"" that are relevant to the input prompt from the llm. and then it asks the model to ""reflect"" over them to generate the final answer. experiments show that far prompting achieves significantly better calibration; it lowers the expected calibration error by 23.5% on our multi-purpose qa tasks. notably, far prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances."
"adaptive chameleon or stubborn sloth: revealing the behavior of large language models in knowledge conflicts. by providing external information to large language models (llms), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of llms' static parametric memory. however, how receptive are llms to such external evidence, especially when the evidence conflicts with their parametric memory? we present the first comprehensive and controlled investigation into the behavior of llms when encountering knowledge conflicts. we propose a systematic framework to elicit high-quality parametric memory from llms and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. our investigation reveals seemingly contradicting behaviors of llms. on the one hand, different from prior wisdom, we find that llms can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. on the other hand, llms also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. these results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented llms. resources are available at https://github.com/osu-nlp-group/llm-knowledge-conflict."
"disentangling memory and reasoning ability in large language models. large language models (llms) have demonstrated strong performance in handling complex tasks requiring both extensive knowledge and reasoning abilities. however, the existing llm inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model's decision-making process unclear and disorganized. this ambiguity can lead to issues such as hallucinations and knowledge forgetting, which significantly impact the reliability of llms in high-stakes domains. in this paper, we propose a new inference paradigm that decomposes the complex inference process into two distinct and clear actions: (1) memory recall: which retrieves relevant knowledge, and (2) reasoning: which performs logical steps based on the recalled knowledge. to facilitate this decomposition, we introduce two special tokens memory and reason, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning. our experiment results show that this decomposition not only improves model performance but also enhances the interpretability of the inference process, enabling users to identify sources of error and refine model responses effectively. the code is available at https://github.com/mingyuj666/disentangling-memory-and-reasoning."
