doc
"zsmerge: zero-shot kv cache compression for memory-efficient long-context llms. the linear growth of key-value (kv) cache memory and quadratic computational in attention mechanisms complexity pose significant bottlenecks for large language models (llms) in long-context processing. while existing kv cache optimization methods address these challenges through token pruning or feature merging, they often incur irreversible information loss or require costly parameter retraining. to this end, we propose zsmerge, a dynamic kv cache compression framework designed for efficient cache management, featuring three key operations: (1) fine-grained memory allocation guided by multi-dimensional token importance metrics at head-level granularity, (2) a residual merging mechanism that preserves critical context through compensated attention scoring, and (3) a zero-shot adaptation mechanism compatible with diverse llm architectures without requiring retraining. zsmerge significantly enhances memory efficiency and inference speed with negligible performance degradation across llms. when applied to llama2-7b, it demonstrates a 20:1 compression ratio for key-value cache retention (reducing memory footprint to 5\% of baseline) while sustaining comparable generation quality, coupled with triple throughput gains at extreme 54k-token contexts that eliminate out-of-memory failures. the code is available at https://github.com/suscom-lab/zsmerge."
"matryoshkakv: adaptive kv compression via trainable orthogonal projection. kv cache has become a de facto technique for the inference of large language models (llms), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. as the size of the model and data grows, the kv cache can quickly become a bottleneck within the system in both storage and memory transfer. to address this, prior studies usually focus on the first three axes of the cache tensors for compression. this paper supplements them, focusing on the feature dimension axis, by utilizing low-rank projection matrices to transform the cache features into spaces with reduced dimensions. we begin by investigating the canonical orthogonal projection method for data compression through principal component analysis (pca). we observe the issue with pca projection where significant performance degradation is observed at low compression rates. to bridge the gap, we propose to directly tune the orthogonal projection matrices with a distillation objective using an elaborate matryoshka training strategy. after training, we adaptively search for the optimal compression rates for various layers and heads given varying compression budgets. compared to previous works, our method can easily embrace pre-trained llms and hold a smooth tradeoff between performance and compression rate. we empirically witness the high data efficiency of our training procedure and find that our method can sustain over 90% performance with an average kv cache compression rate of 60% (and up to 75% in certain extreme scenarios) for popular llms like llama2-7b-base and mistral-7b-v0.3-base."
"ace: exploring activation cosine similarity and variance for accurate and calibration-efficient llm pruning. with the rapid expansion of large language models (llms), the demand for memory and computational resources has grown significantly. recent advances in llm pruning aim to reduce the size and computational cost of these models. however, existing methods often suffer from either suboptimal pruning performance or low time efficiency during the pruning process. in this work, we propose an efficient and effective pruning method that simultaneously achieves high pruning performance and fast pruning speed with improved calibration efficiency. our approach introduces two key innovations: (1) an activation cosine similarity loss-guided pruning metric, which considers the angular deviation of the output activation between the dense and pruned models. (2) an activation variance-guided pruning metric, which helps preserve semantic distinctions in output activations after pruning, enabling effective pruning with shorter input sequences. these two components can be readily combined to enhance llm pruning in both accuracy and efficiency. experimental results show that our method achieves up to an 18% reduction in perplexity and up to 63% decrease in pruning time on prevalent llms such as llama, llama-2, and opt."
