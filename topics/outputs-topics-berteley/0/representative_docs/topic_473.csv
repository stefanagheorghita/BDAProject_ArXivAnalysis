doc
"diverse online feature selection. online feature selection has been an active research area in recent years. we propose a novel diverse online feature selection method based on determinantal point processes (dpp). our model aims to provide diverse features which can be composed in either a supervised or unsupervised framework. the framework aims to promote diversity based on the kernel produced on a feature level, through at most three stages: feature sampling, local criteria and global criteria for feature selection. in the feature sampling, we sample incoming stream of features using conditional dpp. the local criteria is used to assess and select streamed features (i.e. only when they arrive), we use unsupervised scale invariant methods to remove redundant features and optionally supervised methods to introduce label information to assess relevant features. lastly, the global criteria uses regularization methods to select a global optimal subset of features. this three stage procedure continues until there are no more features arriving or some predefined stopping condition is met. we demonstrate based on experiments conducted on that this approach yields better compactness, is comparable and in some instances outperforms other state-of-the-art online feature selection methods."
"a novel memetic feature selection algorithm. feature selection is a problem of finding efficient features among all features in which the final feature set can improve accuracy and reduce complexity. in feature selection algorithms search strategies are key aspects. since feature selection is an np-hard problem; therefore heuristic algorithms have been studied to solve this problem. in this paper, we have proposed a method based on memetic algorithm to find an efficient feature subset for a classification problem. it incorporates a filter method in the genetic algorithm to improve classification performance and accelerates the search in identifying core feature subsets. particularly, the method adds or deletes a feature from a candidate feature subset based on the multivariate feature information. empirical study on commonly data sets of the university of california, irvine shows that the proposed method outperforms existing methods."
"beyond discrete selection: continuous embedding space optimization for generative feature selection. the goal of feature selection - comprising filter, wrapper, and embedded approaches - is to find the optimal feature subset for designated downstream tasks. nevertheless, current feature selection methods are limited by: 1) the selection criteria of these methods are varied for different domains, making them hard to generalize; 2) the selection performance of these approaches drops significantly when processing high-dimensional feature space coupled with small sample size. in light of these challenges, we pose the question: can selected feature subsets be more robust, accurate, and input dimensionality agnostic? in this paper, we reformulate the feature selection problem as a deep differentiable optimization task and propose a new research perspective: conceptualizing discrete feature subsetting as continuous embedding space optimization. we introduce a novel and principled framework that encompasses a sequential encoder, an accuracy evaluator, a sequential decoder, and a gradient ascent optimizer. this comprehensive framework includes four important steps: preparation of features-accuracy training data, deep feature subset embedding, gradient-optimized search, and feature subset reconstruction. specifically, we utilize reinforcement feature selection learning to generate diverse and high-quality training data and enhance generalization. by optimizing reconstruction and accuracy losses, we embed feature selection knowledge into a continuous space using an encoder-evaluator-decoder model structure. we employ a gradient ascent search algorithm to find better embeddings in the learned embedding space. furthermore, we reconstruct feature selection solutions using these embeddings and select the feature subset with the highest performance for downstream tasks as the optimal subset."
