doc
"incorporating type ii error probabilities from independence tests into score-based learning of bayesian network structure. we give a new consistent scoring function for structure learning of bayesian networks. in contrast to traditional approaches to score-based structure learning, such as bdeu or mdl, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. what really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. we prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a bayesian network which is a perfect map for the data generating distribution. although the new score can be used with any search algorithm, in our related uai 2013 paper [bs13], we have given empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to bayesian network structure learning. the present paper contains all details of the proofs of the finite-sample complexity results in [bs13] as well as detailed explanation of the computation of the certain error probabilities called beta-values, whose precomputation and tabulation is necessary for the implementation of the algorithm in [bs13]."
"dirichlet bayesian network scores and the maximum relative entropy principle. a classic approach for learning bayesian networks from data is to identify a maximum a posteriori (map) network structure. in the case of discrete bayesian networks, map networks are selected by maximising one of several possible bayesian dirichlet (bd) scores; the most famous is the bayesian dirichlet equivalent uniform (bdeu) score from heckerman et al (1995). the key properties of bdeu arise from its uniform prior over the parameters of each local distribution in the network, which makes structure learning computationally efficient; it does not require the elicitation of prior knowledge from experts; and it satisfies score equivalence. in this paper we will review the derivation and the properties of bd scores, and of bdeu in particular, and we will link them to the corresponding entropy estimates to study them from an information theoretic perspective. to this end, we will work in the context of the foundational work of giffin and caticha (2007), who showed that bayesian inference can be framed as a particular case of the maximum relative entropy principle. we will use this connection to show that bdeu should not be used for structure learning from sparse data, since it violates the maximum relative entropy principle; and that it is also problematic from a more classic bayesian model selection perspective, because it produces bayes factors that are sensitive to the value of its only hyperparameter. using a large simulation study, we found in our previous work (scutari, 2016) that the bayesian dirichlet sparse (bds) score seems to provide better accuracy in structure learning; in this paper we further show that bds does not suffer from the issues above, and we recommend to use it for sparse data instead of bdeu. finally, will show that these issues are in fact different aspects of the same problem and a consequence of the distributional assumptions of the prior."
"a comparative study of pairwise learning methods based on kernel ridge regression. many machine learning problems can be formulated as predicting labels for a pair of objects. problems of that kind are often referred to as pairwise learning, dyadic prediction or network inference problems. during the last decade kernel methods have played a dominant role in pairwise learning. they still obtain a state-of-the-art predictive performance, but a theoretical analysis of their behavior has been underexplored in the machine learning literature. in this work we review and unify existing kernel-based algorithms that are commonly used in different pairwise learning settings, ranging from matrix filtering to zero-shot learning. to this end, we focus on closed-form efficient instantiations of kronecker kernel ridge regression. we show that independent task kernel ridge regression, two-step kernel ridge regression and a linear matrix filter arise naturally as a special case of kronecker kernel ridge regression, implying that all these methods implicitly minimize a squared loss. in addition, we analyze universality, consistency and spectral filtering properties. our theoretical results provide valuable insights in assessing the advantages and limitations of existing pairwise learning methods."
