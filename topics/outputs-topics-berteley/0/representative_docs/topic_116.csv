doc
"bit: bi-level temporal modeling for efficient supervised action segmentation. we address the task of supervised action segmentation which aims to partition a video into non-overlapping segments, each representing a different action. recent works apply transformers to perform temporal modeling at the frame-level, which suffer from high computational cost and cannot well capture action dependencies over long temporal horizons. to address these issues, we propose an efficient bi-level temporal modeling (bit) framework that learns explicit action tokens to represent action segments, in parallel performs temporal modeling on frame and action levels, while maintaining a low computational cost. our model contains (i) a frame branch that uses convolution to learn frame-level relationships, (ii) an action branch that uses transformer to learn action-level dependencies with a small set of action tokens and (iii) cross-attentions to allow communication between the two branches. we apply and extend a set-prediction objective to allow each action token to represent one or multiple action segments, thus can avoid learning a large number of tokens over long videos with many segments. thanks to the design of our action branch, we can also seamlessly leverage textual transcripts of videos (when available) to help action segmentation by using them to initialize the action tokens. we evaluate our model on four video datasets (two egocentric and two third-person) for action segmentation with and without transcripts, showing that bit significantly improves the state-of-the-art accuracy with much lower computational cost (30 times faster) compared to existing transformer-based methods."
"temporal segment networks: towards good practices for deep action recognition. deep convolutional networks have achieved great success for visual recognition in still images. however, for action recognition in videos, the advantage over traditional methods is not so evident. this paper aims to discover the principles to design effective convnet architectures for action recognition in videos and learn these models given limited training samples. our first contribution is temporal segment network (tsn), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. it combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. the other contribution is our study on a series of good practices in learning convnets on video data with the help of temporal segment network. our approach obtains the state-the-of-art performance on the datasets of hmdb51 ( $ 69.4\% $) and ucf101 ($ 94.2\% $). we also visualize the learned convnet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices."
"temporal action detection by joint identification-verification. temporal action detection aims at not only recognizing action category but also detecting start time and end time for each action instance in an untrimmed video. the key challenge of this task is to accurately classify the action and determine the temporal boundaries of each action instance. in temporal action detection benchmark: thumos 2014, large variations exist in the same action category while many similarities exist in different action categories, which always limit the performance of temporal action detection. to address this problem, we propose to use joint identification-verification network to reduce the intra-action variations and enlarge inter-action differences. the joint identification-verification network is a siamese network based on 3d convnets, which can simultaneously predict the action categories and the similarity scores for the input pairs of video proposal segments. extensive experimental results on the challenging thumos 2014 dataset demonstrate the effectiveness of our proposed method compared to the existing state-of-art methods for temporal action detection in untrimmed videos."
