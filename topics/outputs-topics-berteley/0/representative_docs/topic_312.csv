doc
"joint entropy search for multi-objective bayesian optimization. many real-world problems can be phrased as a multi-objective optimization problem, where the goal is to identify the best set of compromises between the competing objectives. multi-objective bayesian optimization (bo) is a sample efficient strategy that can be deployed to solve these vector-valued optimization problems where access is limited to a number of noisy objective function evaluations. in this paper, we propose a novel information-theoretic acquisition function for bo called joint entropy search (jes), which considers the joint information gain for the optimal set of inputs and outputs. we present several analytical approximations to the jes acquisition function and also introduce an extension to the batch setting. we showcase the effectiveness of this new approach on a range of synthetic and real-world problems in terms of the hypervolume and its weighted variants."
"correcting boundary over-exploration deficiencies in bayesian optimization with virtual derivative sign observations. bayesian optimization (bo) is a global optimization strategy designed to find the minimum of an expensive black-box function, typically defined on a compact subset of $\mathcal{r}^d$, by using a gaussian process (gp) as a surrogate model for the objective. although currently available acquisition functions address this goal with different degree of success, an over-exploration effect of the contour of the search space is typically observed. however, in problems like the configuration of machine learning algorithms, the function domain is conservatively large and with a high probability the global minimum does not sit on the boundary of the domain. we propose a method to incorporate this knowledge into the search process by adding virtual derivative observations in the \gp at the boundary of the search space. we use the properties of gps to impose conditions on the partial derivatives of the objective. the method is applicable with any acquisition function, it is easy to use and consistently reduces the number of evaluations required to optimize the objective irrespective of the acquisition used. we illustrate the benefits of our approach in an extensive experimental comparison."
"batch bayesian optimization via particle gradient flows. bayesian optimisation (bo) methods seek to find global optima of objective functions which are only available as a black-box or are expensive to evaluate. such methods construct a surrogate model for the objective function, quantifying the uncertainty in that surrogate through bayesian inference. objective evaluations are sequentially determined by maximising an acquisition function at each step. however, this ancilliary optimisation problem can be highly non-trivial to solve, due to the non-convexity of the acquisition function, particularly in the case of batch bayesian optimisation, where multiple points are selected in every step. in this work we reformulate batch bo as an optimisation problem over the space of probability measures. we construct a new acquisition function based on multipoint expected improvement which is convex over the space of probability measures. practical schemes for solving this `inner' optimisation problem arise naturally as gradient flows of this objective function. we demonstrate the efficacy of this new method on different benchmark functions and compare with state-of-the-art batch bo methods."
