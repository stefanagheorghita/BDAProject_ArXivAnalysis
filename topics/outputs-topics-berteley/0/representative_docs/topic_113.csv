doc
"multi-gpu graph analytics. we present a single-node, multi-gpu programmable graph processing library that allows programmers to easily extend single-gpu graph algorithms to achieve scalable performance on large graphs with billions of edges. directly using the single-gpu implementations, our design only requires programmers to specify a few algorithm-dependent concerns, hiding most multi-gpu related implementation details. we analyze the theoretical and practical limits to scalability in the context of varying graph primitives and datasets. we describe several optimizations, such as direction optimizing traversal, and a just-enough memory allocation scheme, for better performance and smaller memory consumption. compared to previous work, we achieve best-of-class performance across operations and datasets, including excellent strong and weak scalability on most primitives as we increase the number of gpus in the system."
"accelerating gpu-based out-of-core stencil computation with on-the-fly compression. stencil computation is an important class of scientific applications that can be efficiently executed by graphics processing units (gpus). out-of-core approach helps run large scale stencil codes that process data with sizes larger than the limited capacity of gpu memory. however, the performance of the gpu-based out-of-core stencil computation is always limited by the data transfer between the cpu and gpu. many optimizations have been explored to reduce such data transfer, but the study on the use of on-the-fly compression techniques is far from sufficient. in this study, we propose a method that accelerates the gpu-based out-of-core stencil computation with on-the-fly compression. we introduce a novel data compression approach that solves the data dependency between two contiguous decomposed data blocks. we also modify a widely used gpu-based compression library to support pipelining that overlaps cpu/gpu data transfer with gpu computation. experimental results show that the proposed method achieved a speedup of 1.2x compared the method without compression. moreover, although the precision loss involved by compression increased with the number of time steps, the precision loss was trivial up to 4,320 time steps, demonstrating the usefulness of the proposed method."
"accelerating matrix multiplication: a performance comparison between multi-core cpu and gpu. matrix multiplication is a foundational operation in scientific computing and machine learning, yet its computational complexity makes it a significant bottleneck for large-scale applications. the shift to parallel architectures, primarily multi-core cpus and many-core gpus, is the established solution, and these systems are now ubiquitous from datacenters to consumer laptops. this paper presents a direct, empirical performance analysis of matrix multiplication on a modern, consumer-grade heterogeneous platform. we implemented and benchmarked three versions of the algorithm: a baseline sequential c++ implementation, a parallel version for its multi-core cpu using openmp, and a massively parallel version for its discrete gpu using cuda with shared memory optimizations. the implementations were evaluated with square matrices of varying dimensions, from 128x128 to 4096x4096. our results show that while the parallel cpu provides a consistent speedup of 12-14x over the sequential version, the gpu's performance scales dramatically with problem size. for a 4096x4096 matrix, the gpu implementation achieved a speedup of approximately 593x over the sequential baseline and 45x over the optimized parallel cpu version. these findings quantitatively demonstrate the profound impact of many-core gpu architectures on accelerating data-parallel workloads, underscoring that significant performance gains are readily accessible even on consumer-level hardware."
