doc
"when does in-context learning fall short and why? a study on specification-heavy tasks. in-context learning (icl) has become the default method for using large language models (llms), making the exploration of its limitations and understanding the underlying causes crucial. in this paper, we find that icl falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. the performance of icl on these tasks mostly cannot reach half of the state-of-the-art results. to explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various llms and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. furthermore, we demonstrate that through fine-tuning, llms can achieve decent performance on these tasks, indicating that the failure of icl is not an inherent flaw of llms, but rather a drawback of existing alignment methods that renders llms incapable of handling complicated specification-heavy tasks via icl. to substantiate this, we perform dedicated instruction tuning on llms for these tasks and observe a notable improvement. we hope the analyses in this paper could facilitate advancements in alignment methods enabling llms to meet more sophisticated human demands."
"understanding in-context learning via supportive pretraining data. in-context learning (icl) improves language models' performance on a variety of nlp tasks by simply demonstrating a handful of examples at inference time. it is not well understood why icl ability emerges, as the model has never been specifically trained on such demonstrations. unlike prior work that explores implicit mechanisms behind icl, we study icl via investigating the pretraining data. specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports icl. we observe that a continued pretraining on this small subset significantly improves the model's icl ability, by up to 18%. we then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) the supportive pretraining data to icl do not have a higher domain relevance to downstream tasks. (2) the supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) the supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages icl. our work takes a first step towards understanding icl via analyzing instance-level pretraining data. our insights have a potential to enhance the icl ability of language models by actively guiding the construction of pretraining data in the future."
"investigating the learning behaviour of in-context learning: a comparison with supervised learning. large language models (llms) have shown remarkable capacity for in-context learning (icl), where learning a new task from just a few training examples is done without being explicitly pre-trained. however, despite the success of llms, there has been little understanding of how icl learns the knowledge from the given prompts. in this paper, to make progress toward understanding the learning behaviour of icl, we train the same llms with the same demonstration examples via icl and supervised learning (sl), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. first, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to icl across all model sizes. second, when comparing with sl, we show empirically that icl is less sensitive to label perturbations than sl, and icl gradually attains comparable performance to sl as the model size increases."
