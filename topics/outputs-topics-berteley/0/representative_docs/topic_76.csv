doc
contextual combinatorial semi bandit bandit list classification improve sample complexity sparse reward study problem contextual combinatorial semi bandit input context map subset size collection possible action round learner observe realize reward predict action motivate prototypical application contextual bandit focus regime assume sum reward bound value example recommendation system number product purchase customer significantly small total number available product main result variant problem design algorithm return policy high probability sample complexity underlying finite class sparsity parameter bind improve know bound combinatorial semi bandit regime s lead term bind match correspond information rate imply bandit feedback essentially come cost algorithm computationally efficient give access erm oracle framework generalize list multiclass classification problem bandit feedback see special case binary reward vector special case single label classification correspond s prove sample complexity bind improve recent result scenario additionally consider regret minimization set datum generate adversarially establish regret bind extend result erez et al consider simple single label classification setting
stochastic contextual bandit known reward function sequential decision make problem communication network model contextual bandit problem natural extension know multi armed bandit problem contextual bandit problem time agent observe information context pull arm receive reward arm consider stochastic formulation context reward tuple independently draw unknown distribution trial motivate network application analyze setting reward known non linear function context choose arm current state consider case discrete finite context space propose algorithm prove careful analysis yield regret cumulative reward gap compare distribution aware genie scale logarithmically time linearly number arm optimal context improve exist algorithm regret scale linearly total number arm study continuous context space lipschitz reward function propose algorithm use subroutine reveal novel regret storage trade parametrize tune time horizon allow obtain sub linear regret bound require sub linear storage exploit joint learning contexts regret bound unachievable exist contextual bandit algorithm continuous context space similar performance bound unknown horizon case
non stationary bandit meta learning small set optimal arm study sequential decision problem learner face sequence bandit task task boundary know bandit meta learn setting unknown non stationary bandit setting give integer learner aim compete good subset arm size design algorithm base reduction bandit submodular maximization round comprise task regime large number task small number optimal arm regret setting small simple baseline obtain standard algorithm design non stationary bandit problem bandit meta learn problem fix task length regret algorithm bound additional assumption identifiability optimal arm task bandit meta learn algorithm improved k regret
