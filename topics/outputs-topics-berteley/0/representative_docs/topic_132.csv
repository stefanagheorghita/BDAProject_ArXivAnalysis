doc
"predicting video saliency with object-to-motion cnn and two-layer convolutional lstm. over the past few years, deep neural networks (dnns) have exhibited great success in predicting the saliency of images. however, there are few works that apply dnns to predict the saliency of generic videos. in this paper, we propose a novel dnn-based video saliency prediction method. specifically, we establish a large-scale eye-tracking database of videos (ledov), which provides sufficient data to train the dnn models for predicting video saliency. through the statistical analysis of our ledov database, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. accordingly, we propose an object-to-motion convolutional neural network (om-cnn) to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. we further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. therefore, we develop a two-layer convolutional long short-term memory (2c-lstm) network in our dnn-based method, using the extracted features of om-cnn as the input. consequently, the inter-frame saliency maps of videos can be generated, which consider the transition of attention across video frames. finally, the experimental results show that our method advances the state-of-the-art in video saliency prediction."
"a benchmark dataset and saliency-guided stacked autoencoders for video-based salient object detection. image-based salient object detection (sod) has been extensively studied in the past decades. however, video-based sod is much less explored since there lack large-scale video datasets within which salient objects are unambiguously defined and annotated. toward this end, this paper proposes a video-based sod dataset that consists of 200 videos (64 minutes). in constructing the dataset, we manually annotate all objects and regions over 7,650 uniformly sampled keyframes and collect the eye-tracking data of 23 subjects that free-view all videos. from the user data, we find salient objects in video can be defined as objects that consistently pop-out throughout the video, and objects with such attributes can be unambiguously annotated by combining manually annotated object/region masks with eye-tracking data of multiple subjects. to the best of our knowledge, it is currently the largest dataset for video-based salient object detection. based on this dataset, this paper proposes an unsupervised baseline approach for video-based sod by using saliency-guided stacked autoencoders. in the proposed approach, multiple spatiotemporal saliency cues are first extracted at pixel, superpixel and object levels. with these saliency cues, stacked autoencoders are unsupervisedly constructed which automatically infer a saliency score for each pixel by progressively encoding the high-dimensional saliency cues gathered from the pixel and its spatiotemporal neighbors. experimental results show that the proposed unsupervised approach outperforms 30 state-of-the-art models on the proposed dataset, including 19 image-based & classic (unsupervised or non-deep learning), 6 image-based & deep learning, and 5 video-based & unsupervised. moreover, benchmarking results show that the proposed dataset is very challenging and has the potential to boost the development of video-based sod."
"video saliency detection by 3d convolutional neural networks. different from salient object detection methods for still images, a key challenging for video saliency detection is how to extract and combine spatial and temporal features. in this paper, we present a novel and effective approach for salient object detection for video sequences based on 3d convolutional neural networks. first, we design a 3d convolutional network (conv3dnet) with the input as three video frame to learn the spatiotemporal features for video sequences. then, we design a 3d deconvolutional network (deconv3dnet) to combine the spatiotemporal features to predict the final saliency map for video sequences. experimental results show that the proposed saliency detection model performs better in video saliency prediction compared with the state-of-the-art video saliency detection methods."
