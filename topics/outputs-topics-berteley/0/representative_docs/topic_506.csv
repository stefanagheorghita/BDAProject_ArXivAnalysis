doc
"janus: disaggregating attention and experts for scalable moe inference. large mixture-of-experts (moe) model inference is challenging due to high resource demands and dynamic workloads. existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. in this paper, we propose janus, a scalable moe inference system that disaggregates attention and experts on separate gpu sub-clusters, enabling each module to be managed and scaled independently. janus incorporates three key designs for efficient, disaggregated moe inference. first, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. second, motivated by the memory-bound nature of moe modules, janus introduces a lightweight scheduler and implements it as a gpu kernel to balance the number of activated experts across gpus at minimal overhead, thereby reducing inference latency. third, janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and moe resources to improve overall efficiency. evaluation shows janus achieves up to 3.9 higher pergpu throughput than state-of-the-art systems while meeting per-token latency requirements."
"kvdirect: distributed disaggregated llm inference. large language models (llms) have become the new foundation for many applications, reshaping human society like a storm. disaggregated inference, which separates prefill and decode stages, is a promising approach to improving hardware utilization and service quality. however, due to inefficient inter-node communication, existing systems restrict disaggregated inference to a single node, limiting resource allocation flexibility and reducing service capacity. this paper introduces kvdirect, which optimizes kv cache transfer to enable a distributed disaggregated llm inference. kvdirect achieves this through the following contributions. first, we propose a novel tensor-centric communication mechanism that reduces the synchronization overhead in traditional distributed gpu systems. second, we design a custom communication library to support dynamic gpu resource scheduling and efficient kv cache transfer. third, we introduce a pull-based kv cache transfer strategy that reduces gpu resource idling and improves latency. finally, we implement kvdirect as an open-source llm inference framework. our evaluation demonstrates that kvdirect reduces per-request latency by 55% compared to the baseline across diverse workloads under the same resource constraints."
"characterizing communication patterns in distributed large language model inference. large language models (llms) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. while distributed inference frameworks enable practical deployment of these models, inter-gpu communication creates significant performance constraints that limit service quality in real-world systems. this paper investigates communication dynamics in distributed llm serving-analyzing how various parallelization approaches coordinate data exchange between gpu workers during inference. we study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. these insights offer practical recommendations for selecting appropriate parallelization schemes in production llm services and identify key opportunities for optimizing inference frameworks and communication infrastructure."
