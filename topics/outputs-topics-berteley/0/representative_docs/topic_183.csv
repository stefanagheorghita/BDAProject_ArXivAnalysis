doc
"a decoder-only foundation model for time-series forecasting. motivated by recent advances in large language models for natural language processing (nlp), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities."
"aalf: almost always linear forecasting. recent works for time-series forecasting more and more leverage the high predictive power of deep learning models. with this increase in model complexity, however, comes a lack in understanding of the underlying model decision process, which is problematic for high-stakes application scenarios. at the same time, simple, interpretable forecasting methods such as arima still perform very well, sometimes on-par, with deep learning approaches. we argue that simple models are good enough most of the time, and that forecasting performance could be improved by choosing a deep learning method only for few, important predictions, increasing the overall interpretability of the forecasting process. in this context, we propose a novel online model selection framework which learns to identify these predictions. an extensive empirical study on various real-world datasets shows that our selection methodology performs comparable to state-of-the-art online model selections methods in most cases while being significantly more interpretable. we find that almost always choosing a simple autoregressive linear model for forecasting results in competitive performance, suggesting that the need for opaque black-box models in time-series forecasting might be smaller than recent works would suggest."
"learning latent spaces for domain generalization in time series forecasting. time series forecasting is vital in many real-world applications, yet developing models that generalize well on unseen relevant domains -- such as forecasting web traffic data on new platforms/websites or estimating e-commerce demand in new regions -- remains underexplored. existing forecasting models often struggle with domain shifts in time series data, as the temporal patterns involve complex components like trends, seasonality, etc. while some prior work addresses this by matching feature distributions across domains or disentangling domain-shared features using label information, they fail to reveal insights into the latent temporal dependencies, which are critical for identifying common patterns across domains and achieving generalization. we propose a framework for domain generalization in time series forecasting by mining the latent factors that govern temporal dependencies across domains. our approach uses a decomposition-based architecture with a new conditional $\beta$-variational autoencoder (vae), wherein time series data is first decomposed into trend-cyclical and seasonal components, each modeled independently through separate $\beta$-vae modules. the $\beta$-vae aims to capture disentangled latent factors that control temporal dependencies across domains. we enhance the learning of domain-specific information with a decoder-conditional design and introduce domain regularization to improve the separation of domain-shared and domain-specific latent factors. our proposed method is flexible and can be applied to various time series forecasting models, enabling effective domain generalization with simplicity and efficiency. we validate its effectiveness on five real-world time series datasets, covering web traffic, e-commerce, finance and power consumption, demonstrating improved generalization performance over state-of-the-art methods."
