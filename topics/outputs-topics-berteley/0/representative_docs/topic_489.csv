doc
"steering llms for formal theorem proving. recent advances in automated theorem proving use large language models (llms) to translate informal mathematical statements into formal proofs. however, informal cues are often ambiguous or lack strict logical structure, making it hard for models to interpret them precisely. while existing methods achieve strong performance, little is known about how llms internally represent informal cues, or how these influence proof generation. to address this, we explore \textit{activation steering}, an inference-time intervention that identifies linear directions in residual activations associated with informal reasoning traces and adjusts them to improve proof construction without fine-tuning. this mechanism also yields interpretable information about how reasoning is internally encoded in the activation space of llms. we test our method for generating formal proofs from already-formalized theorems. our contributions are twofold: (1) a novel activation-based intervention for guiding proof synthesis in llms; and (2) demonstration that this intervention improves performance under two decoding strategies (sampling and best-first search) without any further training."
"assessing the reasoning capabilities of llms in the context of evidence-based claim verification. although llms have shown great performance on mathematics and coding related reasoning tasks, the reasoning capabilities of llms regarding other forms of reasoning are still an open problem. here, we examine the issue of reasoning from the perspective of claim verification. we propose a framework designed to break down any claim paired with evidence into atomic reasoning types that are necessary for verification. we use this framework to create recv, the first claim verification benchmark, incorporating real-world claims, to assess the deductive and abductive reasoning capabilities of llms. the benchmark comprises of three datasets, covering reasoning problems of increasing complexity. we evaluate three state-of-the-art proprietary llms under multiple prompt settings. our results show that while llms can address deductive reasoning problems, they consistently fail in cases of abductive reasoning. moreover, we observe that enhancing llms with rationale generation is not always beneficial. nonetheless, we find that generated rationales are semantically similar to those provided by humans, especially in deductive reasoning cases."
"neuro-symbolic integration brings causal and reliable reasoning proofs. two lines of approaches are adopted for complex reasoning with llms. one line of work prompts llms with various reasoning structures, while the structural outputs can be naturally regarded as intermediate reasoning steps. another line of work adopt llm-free declarative solvers to do the reasoning task, rendering higher reasoning accuracy but lacking interpretability due to the black-box nature of the solvers. aiming to resolve the trade-off between answer accuracy and interpretability, we present a simple extension to the latter line of work. specifically, we showcase that the intermediate search logs generated by prolog interpreters can be accessed and interpreted into human-readable reasoning proofs. as long as llms correctly translate problem descriptions into prolog representations, the corresponding reasoning proofs are ensured to be causal and reliable. on two logical reasoning and one arithmetic reasoning datasets, our framework obtains significant improvements in terms of both answer accuracy and reasoning proof accuracy. our code is released at https://github.com/damo-nlp-sg/caring"
