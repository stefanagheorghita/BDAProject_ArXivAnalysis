doc
quantum readout error mitigation deep learning quantum computing device inevitably subject error leverage quantum technology computational benefit practical application quantum algorithm protocol implement reliably noise imperfection noise imperfection limit size quantum circuit realize quantum device develop quantum error mitigation technique require extra qubit gate critical importance work present deep learning base protocol reduce readout error quantum hardware technique base train artificial neural network measurement result obtain experiment simple quantum circuit consist singe qubit gate neural network deep learning non linear noise correct possible exist linear inversion method advantage method exist method demonstrate quantum readout error mitigation experiment perform ibm qubit quantum device
dynamic hypergraph partition quantum circuit hybrid execution quantum algorithm offer exponential speedup classical algorithm range computational problem fundamental mechanism underlie quantum computation require development construction quantum computer device refer nisq noisy intermediate scale quantum device nisq device extremely limited qubit count suffer noise computation problem get bad size circuit increase limit practical use quantum computer modern day application paper focus utilize quantum circuit partition overcome inherent issue nisq device partition quantum circuit small subcircuit allow execution quantum circuit large fit quantum device previous approach quantum circuit partition approach differ work focus hardware aware partitioning optimal graph base partitioning multi processor architecture approach achieve success objective fail scale impact cost noise ultimate goal paper mitigate issue minimize important metric noise time cost achieve use dynamic partitioning practical circuit cutting advantage benefit hybrid execution classical computation alongside quantum hardware approach prove beneficial respect noise classical execution enable reduction noise reduction number qubit require case mixture classical quantum computation require
circuit centric quantum classifier current generation quantum computing technology quantum algorithm require limited number qubit quantum gate robust error suitable design approach variational circuit parameter gate learn approach particularly fruitful application machine learning paper propose low depth variational quantum algorithm supervised learning input feature vector encode amplitude quantum system quantum circuit parametrise single qubit gate single qubit measurement classify input circuit architecture ensure number learnable parameter poly logarithmic input dimension propose quantum classical training scheme analytical gradient model estimate run slightly adapt version variational circuit simulation circuit centric quantum classifier perform standard classical benchmark dataset require dramatically few parameter method evaluate sensitivity classification state preparation parameter noise introduce quantum version dropout regularisation provide graphical representation quantum gate highly symmetric linear layer neural network
