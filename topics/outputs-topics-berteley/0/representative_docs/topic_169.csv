doc
"enhancing low-density eeg-based brain-computer interfaces with similarity-keeping knowledge distillation. electroencephalogram (eeg) has been one of the common neuromonitoring modalities for real-world brain-computer interfaces (bcis) because of its non-invasiveness, low cost, and high temporal resolution. recently, light-weight and portable eeg wearable devices based on low-density montages have increased the convenience and usability of bci applications. however, loss of eeg decoding performance is often inevitable due to reduced number of electrodes and coverage of scalp regions of a low-density eeg montage. to address this issue, we introduce knowledge distillation (kd), a learning mechanism developed for transferring knowledge/information between neural network models, to enhance the performance of low-density eeg decoding. our framework includes a newly proposed similarity-keeping (sk) teacher-student kd scheme that encourages a low-density eeg student model to acquire the inter-sample similarity as in a pre-trained teacher model trained on high-density eeg data. the experimental results validate that our sk-kd framework consistently improves motor-imagery eeg decoding accuracy when number of electrodes deceases for the input eeg data. for both common low-density headphone-like and headband-like montages, our method outperforms state-of-the-art kd methods across various eeg decoding model architectures. as the first kd scheme developed for enhancing eeg decoding, we foresee the proposed sk-kd framework to facilitate the practicality of low-density eeg-based bci in real-world applications."
"learning robust deep visual representations from eeg brain recordings. decoding the human brain has been a hallmark of neuroscientists and artificial intelligence researchers alike. reconstruction of visual images from brain electroencephalography (eeg) signals has garnered a lot of interest due to its applications in brain-computer interfacing. this study proposes a two-stage method where the first step is to obtain eeg-derived features for robust learning of deep representations and subsequently utilize the learned representation for image generation and classification. we demonstrate the generalizability of our feature extraction pipeline across three different datasets using deep-learning architectures with supervised and contrastive learning methods. we have performed the zero-shot eeg classification task to support the generalizability claim further. we observed that a subject invariant linearly separable visual representation was learned using eeg data alone in an unimodal setting that gives better k-means accuracy as compared to a joint representation learning between eeg and images. finally, we propose a novel framework to transform unseen images into the eeg space and reconstruct them with approximation, showcasing the potential for image reconstruction from eeg signals. our proposed image synthesis method from eeg shows 62.9% and 36.13% inception score improvement on the eegcvpr40 and the thoughtviz datasets, which is better than state-of-the-art performance in gan."
"converting your thoughts to texts: enabling brain typing via deep feature learning of eeg signals. an electroencephalography (eeg) based brain computer interface (bci) enables people to communicate with the outside world by interpreting the eeg signals of their brains to interact with devices such as wheelchairs and intelligent robots. more specifically, motor imagery eeg (mi-eeg), which reflects a subjects active intent, is attracting increasing attention for a variety of bci applications. accurate classification of mi-eeg signals while essential for effective operation of bci systems, is challenging due to the significant noise inherent in the signals and the lack of informative correlation between the signals and brain activities. in this paper, we propose a novel deep neural network based learning framework that affords perceptive insights into the relationship between the mi-eeg data and brain activities. we design a joint convolutional recurrent neural network that simultaneously learns robust high-level feature presentations through low-dimensional dense embeddings from raw mi-eeg signals. we also employ an autoencoder layer to eliminate various artifacts such as background activities. the proposed approach has been evaluated extensively on a large- scale public mi-eeg dataset and a limited but easy-to-deploy dataset collected in our lab. the results show that our approach outperforms a series of baselines and the competitive state-of-the- art methods, yielding a classification accuracy of 95.53%. the applicability of our proposed approach is further demonstrated with a practical bci system for typing."
