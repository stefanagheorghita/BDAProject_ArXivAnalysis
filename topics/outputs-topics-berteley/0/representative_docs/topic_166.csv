doc
"distributional individual fairness in clustering. in this paper, we initiate the study of fair clustering that ensures distributional similarity among similar individuals. in response to improving fairness in machine learning, recent papers have investigated fairness in clustering algorithms and have focused on the paradigm of statistical parity/group fairness. these efforts attempt to minimize bias against some protected groups in the population. however, to the best of our knowledge, the alternative viewpoint of individual fairness, introduced by dwork et al. (itcs 2012) in the context of classification, has not been considered for clustering so far. similar to dwork et al., we adopt the individual fairness notion which mandates that similar individuals should be treated similarly for clustering problems. we use the notion of $f$-divergence as a measure of statistical similarity that significantly generalizes the ones used by dwork et al. we introduce a framework for assigning individuals, embedded in a metric space, to probability distributions over a bounded number of cluster centers. the objective is to ensure (a) low cost of clustering in expectation and (b) individuals that are close to each other in a given fairness space are mapped to statistically similar distributions. we provide an algorithm for clustering with $p$-norm objective ($k$-center, $k$-means are special cases) and individual fairness constraints with provable approximation guarantee. we extend this framework to include both group fairness and individual fairness inside the protected groups. finally, we observe conditions under which individual fairness implies group fairness. we present extensive experimental evidence that justifies the effectiveness of our approach."
"understanding fairness surrogate functions in algorithmic fairness. it has been observed that machine learning algorithms exhibit biased predictions against certain population groups. to mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. however, it is intriguing in previous work that such fairness surrogate functions may yield unfair results and high instability. in this work, in order to deeply understand them, taking a widely used fairness definition--demographic parity as an example, we show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. also, the theoretical analysis and experimental results about the gap motivate us that the fairness and stability will be affected by the points far from the decision boundary, which is the large margin points issue investigated in this paper. to address it, we propose the general sigmoid surrogate to simultaneously reduce both the surrogate-fairness gap and the variance, and offer a rigorous fairness and stability upper bound. interestingly, the theory also provides insights into two important issues that deal with the large margin points as well as obtaining a more balanced dataset are beneficial to fairness and stability. furthermore, we elaborate a novel and general algorithm called balanced surrogate, which iteratively reduces the gap to mitigate unfairness. finally, we provide empirical evidence showing that our methods consistently improve fairness and stability while maintaining accuracy comparable to the baselines in three real-world datasets."
"toward unifying group fairness evaluation from a sparsity perspective. ensuring algorithmic fairness remains a significant challenge in machine learning, particularly as models are increasingly applied across diverse domains. while numerous fairness criteria exist, they often lack generalizability across different machine learning problems. this paper examines the connections and differences among various sparsity measures in promoting fairness and proposes a unified sparsity-based framework for evaluating algorithmic fairness. the framework aligns with existing fairness criteria and demonstrates broad applicability to a wide range of machine learning tasks. we demonstrate the effectiveness of the proposed framework as an evaluation metric through extensive experiments on a variety of datasets and bias mitigation methods. this work provides a novel perspective to algorithmic fairness by framing it through the lens of sparsity and social equity, offering potential for broader impact on fairness research and applications."
