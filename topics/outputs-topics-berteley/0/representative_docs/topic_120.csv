doc
"unified learning from demonstrations, corrections, and preferences during physical human-robot interaction. humans can leverage physical interaction to teach robot arms. this physical interaction takes multiple forms depending on the task, the user, and what the robot has learned so far. state-of-the-art approaches focus on learning from a single modality, or combine multiple interaction types by assuming that the robot has prior information about the human's intended task. by contrast, in this paper we introduce an algorithmic formalism that unites learning from demonstrations, corrections, and preferences. our approach makes no assumptions about the tasks the human wants to teach the robot; instead, we learn a reward model from scratch by comparing the human's inputs to nearby alternatives. we first derive a loss function that trains an ensemble of reward models to match the human's demonstrations, corrections, and preferences. the type and order of feedback is up to the human teacher: we enable the robot to collect this feedback passively or actively. we then apply constrained optimization to convert our learned reward into a desired robot trajectory. through simulations and a user study we demonstrate that our proposed approach more accurately learns manipulation tasks from physical human interaction than existing baselines, particularly when the robot is faced with new or unexpected objectives. videos of our user study are available at: https://youtu.be/fsujstyveku"
"data scaling laws in imitation learning for robotic manipulation. data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. in this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. to this end, we conduct a comprehensive empirical study on data scaling in imitation learning. by collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations. throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. the diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. based on these insights, we propose an efficient data collection strategy. with four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90% success rates in novel environments with unseen objects."
"what would you do? acting by learning to predict. we propose to learn tasks directly from visual demonstrations by learning to predict the outcome of human and robot actions on an environment. we enable a robot to physically perform a human demonstrated task without knowledge of the thought processes or actions of the human, only their visually observable state transitions. we evaluate our approach on two table-top, object manipulation tasks and demonstrate generalisation to previously unseen states. our approach reduces the priors required to implement a robot task learning system compared with the existing approaches of learning from demonstration, reinforcement learning and inverse reinforcement learning."
