doc
"faithful to the original: fact aware neural abstractive summarization. unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. while previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. to avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. the dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. experiments on the gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text."
"unsupervised abstractive summarization of bengali text documents. abstractive summarization systems generally rely on large collections of document-summary pairs. however, the performance of abstractive systems remains a challenge due to the unavailability of parallel data for low-resource languages like bengali. to overcome this problem, we propose a graph-based unsupervised abstractive summarization system in the single-document setting for bengali text documents, which requires only a part-of-speech (pos) tagger and a pre-trained language model trained on bengali texts. we also provide a human-annotated dataset with document-summary pairs to evaluate our abstractive model and to support the comparison of future abstractive summarization systems of the bengali language. we conduct experiments on this dataset and compare our system with several well-established unsupervised extractive summarization systems. our unsupervised abstractive summarization model outperforms the baselines without being exposed to any human-annotated reference summaries."
"diverse beam search for increased novelty in abstractive summarization. text summarization condenses a text to a shorter version while retaining the important informations. abstractive summarization is a recent development that generates new phrases, rather than simply copying or rephrasing sentences within the original text. recently neural sequence-to-sequence models have achieved good results in the field of abstractive summarization, which opens new possibilities and applications for industrial purposes. however, most practitioners observe that these models still use large parts of the original text in the output summaries, making them often similar to extractive frameworks. to address this drawback, we first introduce a new metric to measure how much of a summary is extracted from the input text. secondly, we present a novel method, that relies on a diversity factor in computing the neural network loss, to improve the diversity of the summaries generated by any neural abstractive model implementing beam search. finally, we show that this method not only makes the system less extractive, but also improves the overall rouge score of state-of-the-art methods by at least 2 points."
