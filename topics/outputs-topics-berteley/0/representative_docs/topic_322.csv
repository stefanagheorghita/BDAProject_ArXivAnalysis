doc
"a differential approach for gaze estimation. non-invasive gaze estimation methods usually regress gaze directions directly from a single face or eye image. however, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as biases which are subject dependent. therefore, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to his/her actual gaze. in this paper, we introduce a novel image differential method for gaze estimation. we propose to directly train a differential convolutional neural network to predict the gaze differences between two eye input images of the same subject. then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. the assumption is that by allowing the comparison between two eye images, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when using only one calibration sample or when the latter methods are followed by subject specific gaze adaptation."
"gaze-vlm:bridging gaze and vlms through attention regularization for egocentric understanding. eye gaze offers valuable cues about attention, short-term intent, and future actions, making it a powerful signal for modeling egocentric behavior. in this work, we propose a gaze-regularized framework that enhances vlms for two key egocentric understanding tasks: fine-grained future event prediction and current activity understanding. unlike prior approaches that rely solely on visual inputs or use gaze as an auxiliary input signal , our method uses gaze only during training. we introduce a gaze-regularized attention mechanism that aligns model focus with human visual gaze. this design is flexible and modular, allowing it to generalize across multiple vlm architectures that utilize attention. experimental results show that our approach improves semantic prediction scores by up to 11 for future event prediction and around 7 for current activity understanding, compared to the corresponding baseline models trained without gaze regularization. these results highlight the value of gaze-guided training in improving the accuracy and robustness of egocentric vlms. overall, this work establishes a foundation for using human gaze to enhance the predictive capabilities of vlms in real-world scenarios like assistive robots and human-machine collaboration. code and additional information is available at: https://github.com/anupampani/gaze-vlm"
"boosting image-based mutual gaze detection using pseudo 3d gaze. mutual gaze detection, i.e., predicting whether or not two people are looking at each other, plays an important role in understanding human interactions. in this work, we focus on the task of image-based mutual gaze detection, and propose a simple and effective approach to boost the performance by using an auxiliary 3d gaze estimation task during the training phase. we achieve the performance boost without additional labeling cost by training the 3d gaze estimation branch using pseudo 3d gaze labels deduced from mutual gaze labels. by sharing the head image encoder between the 3d gaze estimation and the mutual gaze detection branches, we achieve better head features than learned by training the mutual gaze detection branch alone. experimental results on three image datasets show that the proposed approach improves the detection performance significantly without additional annotations. this work also introduces a new image dataset that consists of 33.1k pairs of humans annotated with mutual gaze labels in 29.2k images."
