doc
"what is your favorite gender, mlm? gender bias evaluation in multilingual masked language models. bias is a disproportionate prejudice in favor of one side against another. due to the success of transformer-based masked language models (mlms) and their impact on many nlp tasks, a systematic evaluation of bias in these models is needed more than ever. while many studies have evaluated gender bias in english mlms, only a few works have been conducted for the task in other languages. this paper proposes a multilingual approach to estimate gender bias in mlms from 5 languages: chinese, english, german, portuguese, and spanish. unlike previous work, our approach does not depend on parallel corpora coupled with english to detect gender bias in other languages using multilingual lexicons. moreover, a novel model-based method is presented to generate sentence pairs for a more robust analysis of gender bias, compared to the traditional lexicon-based method. for each language, both the lexicon-based and model-based methods are applied to create two datasets respectively, which are used to evaluate gender bias in an mlm specifically trained for that language using one existing and 3 new scoring metrics. our results show that the previous approach is data-sensitive and not stable as it does not remove contextual dependencies irrelevant to gender. in fact, the results often flip when different scoring metrics are used on the same dataset, suggesting that gender bias should be studied on a large dataset using multiple evaluation metrics for best practice."
"exposing and correcting the gender bias in image captioning datasets and models. the task of image captioning implicitly involves gender identification. however, due to the gender bias in data, gender identification by an image captioning model suffers. also, the gender-activity bias, owing to the word-by-word prediction, influences other words in the caption prediction, resulting in the well-known problem of label bias. in this work, we investigate gender bias in the coco captioning dataset and show that it engenders not only from the statistical distribution of genders with contexts but also from the flawed annotation by the human annotators. we look at the issues created by this bias in the trained models. we propose a technique to get rid of the bias by splitting the task into 2 subtasks: gender-neutral image captioning and gender classification. by this decoupling, the gender-context influence can be eradicated. we train the gender-neutral image captioning model, which gives comparable results to a gendered model even when evaluating against a dataset that possesses a similar bias as the training data. interestingly, the predictions by this model on images with no humans, are also visibly different from the one trained on gendered captions. we train gender classifiers using the available bounding box and mask-based annotations for the person in the image. this allows us to get rid of the context and focus on the person to predict the gender. by substituting the genders into the gender-neutral captions, we get the final gendered predictions. our predictions achieve similar performance to a model trained with gender, and at the same time are devoid of gender bias. finally, our main result is that on an anti-stereotypical dataset, our model outperforms a popular image captioning model which is trained with gender."
"word embeddings via causal inference: gender bias reducing and semantic information preserving. with widening deployments of natural language processing (nlp) in daily life, inherited social biases from nlp models have become more severe and problematic. previous studies have shown that word embeddings trained on human-generated corpora have strong gender biases that can produce discriminative results in downstream tasks. previous debiasing methods focus mainly on modeling bias and only implicitly consider semantic information while completely overlooking the complex underlying causal structure among bias and semantic components. to address these issues, we propose a novel methodology that leverages a causal inference framework to effectively remove gender bias. the proposed method allows us to construct and analyze the complex causal mechanisms facilitating gender information flow while retaining oracle semantic information within word embeddings. our comprehensive experiments show that the proposed method achieves state-of-the-art results in gender-debiasing tasks. in addition, our methods yield better performance in word similarity evaluation and various extrinsic downstream nlp tasks."
