doc
"multi-modal cooking workflow construction for food recipes. understanding food recipe requires anticipating the implicit causal effects of cooking actions, such that the recipe can be converted into a graph describing the temporal workflow of the recipe. this is a non-trivial task that involves common-sense reasoning. however, existing efforts rely on hand-crafted features to extract the workflow graph from recipes due to the lack of large-scale labeled datasets. moreover, they fail to utilize the cooking images, which constitute an important part of food recipes. in this paper, we build mm-res, the first large-scale dataset for cooking workflow construction, consisting of 9,850 recipes with human-labeled workflow graphs. cooking steps are multi-modal, featuring both text instructions and cooking images. we then propose a neural encoder-decoder model that utilizes both visual and textual information to construct the cooking workflow, which achieved over 20% performance gain over existing hand-crafted baselines."
"learning structural representations for recipe generation and food retrieval. food is significant to human daily life. in this paper, we are interested in learning structural representations for lengthy recipes, that can benefit the recipe generation and food cross-modal retrieval tasks. different from the common vision-language data, here the food images contain mixed ingredients and target recipes are lengthy paragraphs, where we do not have annotations on structure information. to address the above limitations, we propose a novel method to unsupervisedly learn the sentence-level tree structures for the cooking recipes. our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the learned tree structures into the recipe generation and food cross-modal retrieval procedure. our proposed model can produce good-quality sentence-level tree structures and coherent recipes. we achieve the state-of-the-art recipe generation and food cross-modal retrieval performance on the benchmark recipe1m dataset."
"structure-aware generation network for recipe generation from images. sharing food has become very popular with the development of social media. for many real-world applications, people are keen to know the underlying recipes of a food item. in this paper, we are interested in automatically generating cooking instructions for food. we investigate an open research task of generating cooking instructions based on only food images and ingredients, which is similar to the image captioning task. however, compared with image captioning datasets, the target recipes are long-length paragraphs and do not have annotations on structure information. to address the above limitations, we propose a novel framework of structure-aware generation network (sgn) to tackle the food recipe generation task. our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the inferred tree structures with the recipe generation procedure. our proposed model can produce high-quality and coherent recipes, and achieve the state-of-the-art performance on the benchmark recipe1m dataset."
