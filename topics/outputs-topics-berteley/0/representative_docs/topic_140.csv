doc
"turning privacy-preserving mechanisms against federated learning. recently, researchers have successfully employed graph neural networks (gnns) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. in addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global gnn models without collecting sensitive data into a single computation unit. still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. for this reason, experts proposed solutions that combine federated learning with differential privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data. in this paper, we identify a crucial security flaw in such a configuration, and we design an attack capable of deceiving state-of-the-art defenses for federated learning. the proposed attack includes two operating modes, the first one focusing on convergence inhibition (adversarial mode), and the second one aiming at building a deceptive rating injection on the global federated model (backdoor mode). the experimental results show the effectiveness of our attack in both its modes, returning on average 60% performance detriment in all the tests on adversarial mode and fully effective backdoors in 93% of cases for the tests performed on backdoor mode."
"backdoor attacks and defenses in federated learning: survey, challenges and future research directions. federated learning (fl) is a machine learning (ml) approach that allows the use of distributed data without compromising personal privacy. however, the heterogeneous distribution of data among clients in fl can make it difficult for the orchestration server to validate the integrity of local model updates, making fl vulnerable to various threats, including backdoor attacks. backdoor attacks involve the insertion of malicious functionality into a targeted model through poisoned updates from malicious clients. these attacks can cause the global model to misbehave on specific inputs while appearing normal in other cases. backdoor attacks have received significant attention in the literature due to their potential to impact real-world deep learning applications. however, they have not been thoroughly studied in the context of fl. in this survey, we provide a comprehensive survey of current backdoor attack strategies and defenses in fl, including a comprehensive analysis of different approaches. we also discuss the challenges and potential future directions for attacks and defenses in the context of fl."
"poisoning attacks and defenses in federated learning: a survey. federated learning (fl) enables the training of models among distributed clients without compromising the privacy of training datasets, while the invisibility of clients datasets and the training process poses a variety of security threats. this survey provides the taxonomy of poisoning attacks and experimental evaluation to discuss the need for robust fl."
