doc
"correct normalization matters: understanding the effect of normalization on deep neural network models for click-through rate prediction. normalization has become one of the most fundamental components in many deep neural networks for machine learning tasks while deep neural network has also been widely used in ctr estimation field. among most of the proposed deep neural network models, few model utilize normalization approaches. though some works such as deep & cross network (dcn) and neural factorization machine (nfm) use batch normalization in mlp part of the structure, there isn't work to thoroughly explore the effect of the normalization on the dnn ranking systems. in this paper, we conduct a systematic study on the effect of widely used normalization schemas by applying the various normalization approaches to both feature embedding and mlp part in dnn model. extensive experiments are conduct on three real-world datasets and the experiment results demonstrate that the correct normalization significantly enhances model's performance. we also propose a new and effective normalization approaches based on layernorm named variance only layernorm(vo-ln) in this work. a normalization enhanced dnn model named normdnn is also proposed based on the above-mentioned observation. as for the reason why normalization works for dnn models in ctr estimation, we find that the variance of normalization plays the main role and give an explanation in this work."
"mode normalization. normalization methods are a central building block in the deep learning toolbox. they accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules. when learning from multi-modal distributions, the effectiveness of batch normalization (bn), arguably the most prominent normalization method, is reduced. as a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features. we demonstrate that our method outperforms bn and other widely used normalization techniques in several experiments, including single and multi-task datasets."
"normalizing the normalizers: comparing and extending network normalization schemes. normalization techniques have only recently begun to be exploited in supervised learning tasks. batch normalization exploits mini-batch statistics to normalize the activations. this was shown to speed up training and result in better models. however its success has been very limited when dealing with recurrent neural networks. on the other hand, layer normalization normalizes the activations across all activities within a layer. this was shown to work well in the recurrent setting. in this paper we propose a unified view of normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. we demonstrate the effectiveness of our unified divisive normalization framework in the context of convolutional neural nets and recurrent neural networks, showing improvements over baselines in image classification, language modeling as well as super-resolution."
