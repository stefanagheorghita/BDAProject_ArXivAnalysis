doc
"enhancing bidirectional sign language communication: integrating yolov8 and nlp for real-time gesture recognition & translation. the primary concern of this research is to take american sign language (asl) data through real time camera footage and be able to convert the data and information into text. adding to that, we are also putting focus on creating a framework that can also convert text into sign language in real time which can help us break the language barrier for the people who are in need. in this work, for recognising american sign language (asl), we have used the you only look once(yolo) model and convolutional neural network (cnn) model. yolo model is run in real time and automatically extracts discriminative spatial-temporal characteristics from the raw video stream without the need for any prior knowledge, eliminating design flaws. the cnn model here is also run in real time for sign language detection. we have introduced a novel method for converting text based input to sign language by making a framework that will take a sentence as input, identify keywords from that sentence and then show a video where sign language is performed with respect to the sentence given as input in real time. to the best of our knowledge, this is a rare study to demonstrate bidirectional sign language communication in real time in the american sign language (asl)."
"word separation in continuous sign language using isolated signs and post-processing. . continuous sign language recognition (cslr) is a long challenging task in computer vision due to the difficulties in detecting the explicit boundaries between the words in a sign sentence. to deal with this challenge, we propose a two-stage model. in the first stage, the predictor model, which includes a combination of cnn, svd, and lstm, is trained with the isolated signs. in the second stage, we apply a post-processing algorithm to the softmax outputs obtained from the first part of the model in order to separate the isolated signs in the continuous signs. while the proposed model is trained on the isolated sign classes with similar frame numbers, it is evaluated on the continuous sign videos with a different frame length per each isolated sign class. due to the lack of a large dataset, including both the sign sequences and the corresponding isolated signs, two public datasets in isolated sign language recognition (islr), rks-persiansign and asllvd, are used for evaluation. results of the continuous sign videos confirm the efficiency of the proposed model to deal with isolated sign boundaries detection."
"pose-based sign language spotting via an end-to-end encoder architecture. automatic sign language recognition (aslr) has emerged as a vital field for bridging the gap between deaf and hearing communities. however, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. we define this novel task as sign language spotting. in this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. by focusing on pose representations instead of raw rgb frames, our method significantly reduces computational cost and mitigates visual noise. we evaluate our approach on the word presence prediction dataset from the wslp 2025 shared task, achieving 61.88\% accuracy and 60.00\% f1-score. these results demonstrate the effectiveness of our pose-based framework for sign language spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. code is available at https://github.com/ebimojohnny/pose-based-sign-language-spotting"
