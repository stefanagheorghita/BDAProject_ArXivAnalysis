doc
"hamiltonian assisted metropolis sampling. various markov chain monte carlo (mcmc) methods are studied to improve upon random walk metropolis sampling, for simulation from complex distributions. examples include metropolis-adjusted langevin algorithms, hamiltonian monte carlo, and other recent algorithms related to underdamped langevin dynamics. we propose a broad class of irreversible sampling algorithms, called hamiltonian assisted metropolis sampling (hams), and develop two specific algorithms with appropriate tuning and preconditioning strategies. our hams algorithms are designed to achieve two distinctive properties, while using an augmented target density with momentum as an auxiliary variable. one is generalized detailed balance, which induces an irreversible exploration of the target. the other is a rejection-free property, which allows our algorithms to perform satisfactorily with relatively large step sizes. furthermore, we formulate a framework of generalized metropolis--hastings sampling, which not only highlights our construction of hams at a more abstract level, but also facilitates possible further development of irreversible mcmc algorithms. we present several numerical experiments, where the proposed algorithms are found to consistently yield superior results among existing ones."
"spreadnuts -- moderate dynamic extension of paths for no-u-turn sampling & partitioning visited regions. markov chain monte carlo (mcmc) methods have existed for a long time and the field is well-explored. the purpose of mcmc methods is to approximate a distribution through repeated sampling; most mcmc algorithms exhibit asymptotically optimal behavior in that they converge to the true distribution at the limit. however, what differentiates these algorithms are their practical convergence guarantees and efficiency. while a sampler may eventually approximate a distribution well, because it is used in the real world it is necessary that the point at which the sampler yields a good estimate of the distribution is reachable in a reasonable amount of time. similarly, if it is computationally difficult or intractable to produce good samples from a distribution for use in estimation, then there is no real-world utility afforded by the sampler. thus, most mcmc methods these days focus on improving efficiency and speeding up convergence. however, many mcmc algorithms suffer from random walk behavior and often only mitigate such behavior as outright erasing random walks is difficult. hamiltonian monte carlo (hmc) is a class of mcmc methods that theoretically exhibit no random walk behavior because of properties related to hamiltonian dynamics. this paper introduces modifications to a specific hmc algorithm known as the no-u-turn sampler (nuts) that aims to explore the sample space faster than nuts, yielding a sampler that has faster convergence to the true distribution than nuts."
"markov chain monte carlo methods for lattice gaussian sampling:convergence analysis and enhancement. sampling from lattice gaussian distribution has emerged as an important problem in coding, decoding and cryptography. in this paper, the classic gibbs algorithm from markov chain monte carlo (mcmc) methods is demonstrated to be geometrically ergodic for lattice gaussian sampling, which means the markov chain arising from it converges exponentially fast to the stationary distribution. meanwhile, the exponential convergence rate of markov chain is also derived through the spectral radius of forward operator. then, a comprehensive analysis regarding to the convergence rate is carried out and two sampling schemes are proposed to further enhance the convergence performance. the first one, referred to as metropolis-within-gibbs (mwg) algorithm, improves the convergence by refining the state space of the univariate sampling. on the other hand, the blocked strategy of gibbs algorithm, which performs the sampling over multivariate at each markov move, is also shown to yield a better convergence rate than the traditional univariate sampling. in order to perform blocked sampling efficiently, gibbs-klein (gk) algorithm is proposed, which samples block by block using klein's algorithm. furthermore, the validity of gk algorithm is demonstrated by showing its ergodicity. simulation results based on mimo detections are presented to confirm the convergence gain brought by the proposed gibbs sampling schemes."
