doc
"gans with variational entropy regularizers: applications in mitigating the mode-collapse issue. building on the success of deep learning, generative adversarial networks (gans) provide a modern approach to learn a probability distribution from observed samples. gans are often formulated as a zero-sum game between two sets of functions; the generator and the discriminator. although gans have shown great potentials in learning complex distributions such as images, they often suffer from the mode collapse issue where the generator fails to capture all existing modes of the input distribution. as a consequence, the diversity of generated samples is lower than that of the observed ones. to tackle this issue, we take an information-theoretic approach and maximize a variational lower bound on the entropy of the generated samples to increase their diversity. we call this approach gans with variational entropy regularizers (gan+ver). existing remedies for the mode collapse issue in gans can be easily coupled with our proposed variational entropy regularization. through extensive experimentation on standard benchmark datasets, we show all the existing evaluation metrics highlighting difference of real and generated samples are significantly improved with gan+ver."
"convergence problems with generative adversarial networks (gans). generative adversarial networks (gans) are a novel approach to generative modelling, a task whose goal it is to learn a distribution of real data points. they have often proved difficult to train: gans are unlike many techniques in machine learning, in that they are best described as a two-player game between a discriminator and generator. this has yielded both unreliability in the training process, and a general lack of understanding as to how gans converge, and if so, to what. the purpose of this dissertation is to provide an account of the theory of gans suitable for the mathematician, highlighting both positive and negative results. this involves identifying the problems when training gans, and how topological and game-theoretic perspectives of gans have contributed to our understanding and improved our techniques in recent years."
"evolutionary generative adversarial networks. generative adversarial networks (gan) have been effective for learning generative models for real-world data. however, existing gans (gan and its variants) tend to suffer from training problems such as instability and mode collapse. in this paper, we propose a novel gan framework called evolutionary generative adversarial networks (e-gan) for stable gan training and improved generative performance. unlike existing gans, which employ a pre-defined adversarial objective function alternately training a generator and a discriminator, we utilize different adversarial training objectives as mutation operations and evolve a population of generators to adapt to the environment (i.e., the discriminator). we also utilize an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. in this way, e-gan overcomes the limitations of an individual adversarial training objective and always preserves the best offspring, contributing to progress in and the success of gans. experiments on several datasets demonstrate that e-gan achieves convincing generative performance and reduces the training problems inherent in existing gans."
