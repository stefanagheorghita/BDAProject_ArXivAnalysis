doc
"ophnet: a large-scale video benchmark for ophthalmic surgical workflow understanding. surgical scene perception via videos is critical for advancing robotic surgery, telesurgery, and ai-assisted surgery, particularly in ophthalmology. however, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis. existing datasets face challenges such as small scale, lack of diversity in surgery and phase categories, and absence of time-localized annotations. these limitations impede action understanding and model generalization validation in complex and diverse real-world surgical scenarios. to address this gap, we introduce ophnet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding. ophnet features: 1) a diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 fine-grained operations. 2) sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability. 3) time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows. with approximately 285 hours of surgical videos, ophnet is about 20 times larger than the largest existing surgical workflow analysis benchmark. code and dataset are available at: https://minghu0830.github.io/ophnet-benchmark/."
"compass: a formal framework and aggregate dataset for generalized surgical procedure modeling. purpose: we propose a formal framework for the modeling and segmentation of minimally-invasive surgical tasks using a unified set of motion primitives (mps) to enable more objective labeling and the aggregation of different datasets. methods: we model dry-lab surgical tasks as finite state machines, representing how the execution of mps as the basic surgical actions results in the change of surgical context, which characterizes the physical interactions among tools and objects in the surgical environment. we develop methods for labeling surgical context based on video data and for automatic translation of context to mp labels. we then use our framework to create the context and motion primitive aggregate surgical set (compass), including six dry-lab surgical tasks from three publicly-available datasets (jigsaws, desk, and rosma), with kinematic and video data and context and mp labels. results: our context labeling method achieves near-perfect agreement between consensus labels from crowd-sourcing and expert surgeons. segmentation of tasks to mps results in the creation of the compass dataset that nearly triples the amount of data for modeling and analysis and enables the generation of separate transcripts for the left and right tools. conclusion: the proposed framework results in high quality labeling of surgical data based on context and fine-grained mps. modeling surgical tasks with mps enables the aggregation of different datasets and the separate analysis of left and right hands for bimanual coordination assessment. our formal framework and aggregate dataset can support the development of explainable and multi-granularity models for improved surgical process analysis, skill assessment, error detection, and autonomy."
"large-scale self-supervised video foundation model for intelligent surgery. computer-assisted intervention (cai) has the potential to revolutionize modern surgery, with surgical scene understanding serving as a critical component in supporting decision-making, improving procedural efficacy, and ensuring intraoperative safety. while existing ai-driven approaches alleviate annotation burdens via self-supervised spatial representation learning, their lack of explicit temporal modeling during pre-training fundamentally restricts the capture of dynamic surgical contexts, resulting in incomplete spatiotemporal understanding. in this work, we introduce the first video-level surgical pre-training framework that enables joint spatiotemporal representation learning from large-scale surgical video data. to achieve this, we constructed a large-scale surgical video dataset comprising 3,650 videos and approximately 3.55 million frames, spanning more than 20 surgical procedures and over 10 anatomical structures. building upon this dataset, we propose surgvista (surgical video-level spatial-temporal architecture), a reconstruction-based pre-training method that captures intricate spatial structures and temporal dynamics through joint spatiotemporal modeling. additionally, surgvista incorporates image-level knowledge distillation guided by a surgery-specific expert to enhance the learning of fine-grained anatomical and semantic features. to validate its effectiveness, we established a comprehensive benchmark comprising 13 video-level datasets spanning six surgical procedures across four tasks. extensive experiments demonstrate that surgvista consistently outperforms both natural- and surgical-domain pre-trained models, demonstrating strong potential to advance intelligent surgical systems in clinically meaningful scenarios."
