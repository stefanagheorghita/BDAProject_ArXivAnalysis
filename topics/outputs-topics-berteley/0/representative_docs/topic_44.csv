doc
efficient random coordinate descent algorithm large scale structured nonconvex optimization paper analyze new method solve nonconvex optimization problem objective function form sum term nonconvex smooth convex simple structure know consider case unconstrained linearly constrain nonconvex problem optimization problem structure propose random coordinate descent algorithm analyze convergence property general case objective function nonconvex composite prove asymptotic convergence sequence generate algorithm stationary point sublinear rate convergence expectation optimality measure additionally objective function satisfy error bind condition derive local linear rate convergence expect value objective function present extensive numerical experiment evaluate performance algorithm comparison state art method
rich theory convex constrain optimization reduce projection improve rate paper focus convex constrain optimization problem solution subject convex inequality constraint particular aim challenge problem projection constrain domain linear optimization inequality constraint time consuming render project gradient method conditional gradient method frank wolfe algorithm expensive paper develop projection reduce optimization algorithm smooth non smooth optimization improved convergence rate certain regularity condition constraint function present general theory optimization projection application smooth optimization projection yield iteration complexity improve iteration complexity establish non smooth optimization reduce strong convexity introduce local error bind condition develop fast algorithm non strongly convex optimization price logarithmic number projection particular achieve iteration complexity non smooth optimization smooth optimization appear local error bind condition characterize functional local growth rate optimal solution novel application solve constrain minimization problem positive semi definite constrain distance metric learning problem demonstrate propose algorithm achieve significant speed compare previous algorithm
optimization order algorithm note focus minimization convex functional order optimization method fundamental area applied mathematic engineering primary goal document introduce analyze classical order optimization algorithm aim provide reader practical theoretical understanding algorithm converge minimizer convex function main algorithm cover note include gradient descent forward backward splitting douglas rachford splitting alternate direction method multiplier admm primal dual algorithm algorithm fall class order method involve gradient subdifferential order derivative function optimize method provide convergence theorem precise assumption condition convergence hold accompany complete proof convex optimization final manuscript extend analysis nonconvex problem discuss convergence behavior order method broad assumption contextualize theory include selection practical example illustrate algorithm apply different image processing problem
