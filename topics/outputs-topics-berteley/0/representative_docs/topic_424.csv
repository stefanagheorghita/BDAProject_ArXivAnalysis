doc
"grad-nav: efficiently learning visual drone navigation with gaussian radiance fields and differentiable dynamics. autonomous visual navigation is an essential element in robot autonomy. reinforcement learning (rl) offers a promising policy training paradigm. however existing rl methods suffer from high sample complexity, poor sim-to-real transfer, and limited runtime adaptability to navigation scenarios not seen during training. these problems are particularly challenging for drones, with complex nonlinear and unstable dynamics, and strong dynamic coupling between control and perception. in this paper, we propose a novel framework that integrates 3d gaussian splatting (3dgs) with differentiable deep reinforcement learning (ddrl) to train vision-based drone navigation policies. by leveraging high-fidelity 3d scene representations and differentiable simulation, our method improves sample efficiency and sim-to-real transfer. additionally, we incorporate a context-aided estimator network (cenet) to adapt to environmental variations at runtime. moreover, by curriculum training in a mixture of different surrounding environments, we achieve in-task generalization, the ability to solve new instances of a task not seen during training. drone hardware experiments demonstrate our method's high training efficiency compared to state-of-the-art rl methods, zero shot sim-to-real transfer for real robot deployment without fine tuning, and ability to adapt to new instances within the same task class (e.g. to fly through a gate at different locations with different distractors in the environment). our simulator and training framework are open-sourced at: https://github.com/qianzhong-chen/grad_nav."
"sim-real joint reinforcement transfer for 3d indoor navigation. there has been an increasing interest in 3d indoor navigation, where a robot in an environment moves to a target according to an instruction. to deploy a robot for navigation in the physical world, lots of training data is required to learn an effective policy. it is quite labour intensive to obtain sufficient real environment data for training robots while synthetic data is much easier to construct by rendering. though it is promising to utilize the synthetic environments to facilitate navigation training in the real world, real environment are heterogeneous from synthetic environment in two aspects. first, the visual representation of the two environments have significant variances. second, the houseplans of these two environments are quite different. therefore two types of information, i.e. visual representation and policy behavior, need to be adapted in the reinforcement model. the learning procedure of visual representation and that of policy behavior are presumably reciprocal. we propose to jointly adapt visual representation and policy behavior to leverage the mutual impacts of environment and policy. specifically, our method employs an adversarial feature adaptation model for visual representation transfer and a policy mimic strategy for policy behavior imitation. experiment shows that our method outperforms the baseline by 19.47% without any additional human annotations."
"on embodied visual navigation in real environments through habitat. visual navigation models based on deep learning can learn effective policies when trained on large amounts of visual observations through reinforcement learning. unfortunately, collecting the required experience in the real world requires the deployment of a robotic platform, which is expensive and time-consuming. to deal with this limitation, several simulation platforms have been proposed in order to train visual navigation policies on virtual environments efficiently. despite the advantages they offer, simulators present a limited realism in terms of appearance and physical dynamics, leading to navigation policies that do not generalize in the real world. in this paper, we propose a tool based on the habitat simulator which exploits real world images of the environment, together with sensor and actuator noise models, to produce more realistic navigation episodes. we perform a range of experiments to assess the ability of such policies to generalize using virtual and real-world images, as well as observations transformed with unsupervised domain adaptation approaches. we also assess the impact of sensor and actuation noise on the navigation performance and investigate whether it allows to learn more robust navigation policies. we show that our tool can effectively help to train and evaluate navigation policies on real-world observations without running navigation pisodes in the real world."
