doc
"a signal propagation perspective for pruning neural networks at initialization. network pruning is a promising avenue for compressing deep neural networks. a typical approach to pruning starts by training a model and then removing redundant parameters while minimizing the impact on what is learned. alternatively, a recent approach shows that pruning can be done at initialization prior to training, based on a saliency criterion called connection sensitivity. however, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. in this work, by noting connection sensitivity as a form of gradient, we formally characterize initialization conditions to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. moreover, we analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. our modifications to the existing pruning at initialization method lead to improved results on all tested network models for image classification tasks. furthermore, we empirically study the effect of supervision for pruning and demonstrate that our signal propagation perspective, combined with unsupervised pruning, can be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures."
"does unsupervised architecture representation learning help neural architecture search?. existing neural architecture search (nas) methods either encode neural architectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. despite the widespread use, architecture representations learned in nas are still poorly understood. we observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. in this work, we find empirically that pre-training architecture representations using only neural architectures without their accuracies as labels considerably improve the downstream architecture search efficiency. to explain these observations, we visualize how unsupervised architecture representation learning better encourages neural architectures with similar connections and operators to cluster together. this helps to map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably benefits diverse downstream search strategies."
"search space adaptation for differentiable neural architecture search in image classification. as deep neural networks achieve unprecedented performance in various tasks, neural architecture search (nas), a research field for designing neural network architectures with automated processes, is actively underway. more recently, differentiable nas has a great impact by reducing the search cost to the level of training a single network. besides, the search space that defines candidate architectures to be searched directly affects the performance of the final architecture. in this paper, we propose an adaptation scheme of the search space by introducing a search scope. the effectiveness of proposed method is demonstrated with proxylessnas for the image classification task. furthermore, we visualize the trajectory of architecture parameter updates and provide insights to improve the architecture search."
