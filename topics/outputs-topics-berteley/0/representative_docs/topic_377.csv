doc
"hfel: joint edge association and resource allocation for cost-efficient hierarchical federated edge learning. federated learning (fl) has been proposed as an appealing approach to handle data privacy issue of mobile devices compared to conventional machine learning at the remote cloud with raw user data uploading. by leveraging edge servers as intermediaries to perform partial model aggregation in proximity and relieve core network transmission overhead, it enables great potentials in low-latency and energy-efficient fl. hence we introduce a novel hierarchical federated edge learning (hfel) framework in which model aggregation is partially migrated to edge servers from the cloud. we further formulate a joint computation and communication resource allocation and edge association problem for device users under hfel framework to achieve global cost minimization. to solve the problem, we propose an efficient resource scheduling algorithm in the hfel framework. it can be decomposed into two subproblems: \emph{resource allocation} given a scheduled set of devices for each edge server and \emph{edge association} of device users across all the edge servers. with the optimal policy of the convex resource allocation subproblem for a set of devices under a single edge server, an efficient edge association strategy can be achieved through iterative global cost reduction adjustment process, which is shown to converge to a stable system point. extensive performance evaluations demonstrate that our hfel framework outperforms the proposed benchmarks in global cost saving and achieves better training performance compared to conventional federated learning."
"device scheduling for relay-assisted over-the-air aggregation in federated learning. federated learning (fl) leverages data distributed at the edge of the network to enable intelligent applications. the efficiency of fl can be improved by using over-the-air computation (aircomp) technology in the process of gradient aggregation. in this paper, we propose a relay-assisted large-scale fl framework, and investigate the device scheduling problem in relay-assisted fl systems under the constraints of power consumption and mean squared error (mse). we formulate a joint device scheduling, and power allocation problem to maximize the number of scheduled devices. we solve the resultant non-convex optimization problem by transforming the optimization problem into multiple sparse optimization problems. by the proposed device scheduling algorithm, these sparse sub-problems are solved and the maximum number of federated learning edge devices is obtained. the simulation results demonstrate the effectiveness of the proposed scheme as compared with other benchmark schemes."
"gradient and channel aware dynamic scheduling for over-the-air computation in federated edge learning systems. to satisfy the expected plethora of computation-heavy applications, federated edge learning (feel) is a new paradigm featuring distributed learning to carry the capacities of low-latency and privacy-preserving. to further improve the efficiency of wireless data aggregation and model learning, over-the-air computation (aircomp) is emerging as a promising solution by using the superposition characteristics of wireless channels. however, the fading and noise of wireless channels can cause aggregate distortions in aircomp enabled federated learning. in addition, the quality of collected data and energy consumption of edge devices may also impact the accuracy and efficiency of model aggregation as well as convergence. to solve these problems, this work proposes a dynamic device scheduling mechanism, which can select qualified edge devices to transmit their local models with a proper power control policy so as to participate the model training at the server in federated learning via aircomp. in this mechanism, the data importance is measured by the gradient of local model parameter, channel condition and energy consumption of the device jointly. in particular, to fully use distributed datasets and accelerate the convergence rate of federated learning, the local updates of unselected devices are also retained and accumulated for future potential transmission, instead of being discarded directly. furthermore, the lyapunov drift-plus-penalty optimization problem is formulated for searching the optimal device selection strategy. simulation results validate that the proposed scheduling mechanism can achieve higher test accuracy and faster convergence rate, and is robust against different channel conditions."
