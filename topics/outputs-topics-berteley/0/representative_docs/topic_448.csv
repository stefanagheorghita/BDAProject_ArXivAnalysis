doc
"contrastalign: toward robust bev feature alignment via contrastive learning for multi-modal 3d object detection. in the field of 3d object detection tasks, fusing heterogeneous features from lidar and camera sensors into a unified bird's eye view (bev) representation is a widely adopted paradigm. however, existing methods often suffer from imprecise sensor calibration, leading to feature misalignment in lidar-camera bev fusion. moreover, such inaccuracies cause errors in depth estimation for the camera branch, aggravating misalignment between lidar and camera bev features. in this work, we propose a novel contrastalign approach that utilizes contrastive learning to enhance the alignment of heterogeneous modalities, thereby improving the robustness of the fusion process. specifically, our approach comprises three key components: (1) the l-instance module, which extracts lidar instance features within the lidar bev features; (2) the c-instance module, which predicts camera instance features through region of interest (roi) pooling on the camera bev features; (3) the instancefusion module, which employs contrastive learning to generate consistent instance features across hefterogeneous modalities. subsequently, we use graph matching to calculate the similarity between the neighboring camera instance features and the similarity instance features to complete the alignment of instance features. our method achieves sota performance, with an map of 71.5%, surpassing graphbev by 1.4% on the nuscenes val set. importantly, our method excels bevfusion under conditions with spatial & temporal misalignment noise, improving map by 1.4% and 11.1% on nuscenes dataset. notably, on the argoverse2 dataset, contrastalign outperforms graphbev by 1.0% in map, indicating that the farther the distance, the more severe the feature misalignment and the more effective."
"lightweight spatial embedding for vision-based 3d occupancy prediction. occupancy prediction has garnered increasing attention in recent years for its comprehensive fine-grained environmental representation and strong generalization to open-set objects. however, cumbersome voxel features and 3d convolution operations inevitably introduce large overheads in both memory and computation, obstructing the deployment of occupancy prediction approaches in real-time autonomous driving systems. although some methods attempt to efficiently predict 3d occupancy from 2d bird's-eye-view (bev) features through the channel-to-height mechanism, bev features are insufficient to store all the height information of the scene, which limits performance. this paper proposes lightocc, an innovative 3d occupancy prediction framework that leverages lightweight spatial embedding to effectively supplement the height clues for the bev-based representation while maintaining its deployability. firstly, global spatial sampling is used to obtain the single-channel occupancy from multi-view depth distribution. spatial-to-channel mechanism then takes the arbitrary spatial dimension of single-channel occupancy as the feature dimension and extracts tri-perspective views (tpv) embeddings by 2d convolution. finally, tpv embeddings will interact with each other by lightweight tpv interaction module to obtain the spatial embedding that is optimal supplementary to bev features. sufficient experimental results show that lightocc significantly increases the prediction accuracy of the baseline and achieves state-of-the-art performance on the occ3d-nuscenes benchmark."
"bevpose: unveiling scene semantics through pose-guided multi-modal bev alignment. in the field of autonomous driving and mobile robotics, there has been a significant shift in the methods used to create bird's eye view (bev) representations. this shift is characterised by using transformers and learning to fuse measurements from disparate vision sensors, mainly lidar and cameras, into a 2d planar ground-based representation. however, these learning-based methods for creating such maps often rely heavily on extensive annotated data, presenting notable challenges, particularly in diverse or non-urban environments where large-scale datasets are scarce. in this work, we present bevpose, a framework that integrates bev representations from camera and lidar data, using sensor pose as a guiding supervisory signal. this method notably reduces the dependence on costly annotated data. by leveraging pose information, we align and fuse multi-modal sensory inputs, facilitating the learning of latent bev embeddings that capture both geometric and semantic aspects of the environment. our pretraining approach demonstrates promising performance in bev map segmentation tasks, outperforming fully-supervised state-of-the-art methods, while necessitating only a minimal amount of annotated data. this development not only confronts the challenge of data efficiency in bev representation learning but also broadens the potential for such techniques in a variety of domains, including off-road and indoor environments."
