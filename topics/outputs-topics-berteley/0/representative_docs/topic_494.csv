doc
"realistic speech-driven facial animation with gans. speech-driven facial animation is the process that automatically synthesizes talking characters based on speech signals. the majority of work in this domain creates a mapping from audio features to visual features. this approach often requires post-processing using computer graphics techniques to produce realistic albeit subject dependent results. we present an end-to-end system that generates videos of a talking head, using only a still image of a person and an audio clip containing speech, without relying on handcrafted intermediate features. our method generates videos which have (a) lip movements that are in sync with the audio and (b) natural facial expressions such as blinks and eyebrow movements. our temporal gan uses 3 discriminators focused on achieving detailed frames, audio-visual synchronization, and realistic expressions. we quantify the contribution of each component in our model using an ablation study and we provide insights into the latent representation of the model. the generated videos are evaluated based on sharpness, reconstruction quality, lip-reading accuracy, synchronization as well as their ability to generate natural blinks."
"nerf-3dtalker: neural radiance field with 3d prior aided audio disentanglement for talking head synthesis. talking head synthesis is to synthesize a lip-synchronized talking head video using audio. recently, the capability of nerf to enhance the realism and texture details of synthesized talking heads has attracted the attention of researchers. however, most current nerf methods based on audio are exclusively concerned with the rendering of frontal faces. these methods are unable to generate clear talking heads in novel views. another prevalent challenge in current 3d talking head synthesis is the difficulty in aligning acoustic and visual spaces, which often results in suboptimal lip-syncing of the generated talking heads. to address these issues, we propose neural radiance field with 3d prior aided audio disentanglement for talking head synthesis (nerf-3dtalker). specifically, the proposed method employs 3d prior information to synthesize clear talking heads with free views. additionally, we propose a 3d prior aided audio disentanglement module, which is designed to disentangle the audio into two distinct categories: features related to 3d awarded speech movements and features related to speaking style. moreover, to reposition the generated frames that are distant from the speaker's motion space in the real space, we have devised a local-global standardized space. this method normalizes the irregular positions in the generated frames from both global and local semantic perspectives. through comprehensive qualitative and quantitative experiments, it has been demonstrated that our nerf-3dtalker outperforms state-of-the-art in synthesizing realistic talking head videos, exhibiting superior image quality and lip synchronization. project page: https://nerf-3dtalker.github.io/nerf-3dtalker."
"talking face generation by conditional recurrent adversarial network. given an arbitrary face image and an arbitrary speech clip, the proposed work attempts to generating the talking face video with accurate lip synchronization while maintaining smooth transition of both lip and facial movement over the entire video clip. existing works either do not consider temporal dependency on face images across different video frames thus easily yielding noticeable/abrupt facial and lip movement or are only limited to the generation of talking face video for a specific person thus lacking generalization capacity. we propose a novel conditional video generation network where the audio input is treated as a condition for the recurrent adversarial network such that temporal dependency is incorporated to realize smooth transition for the lip and facial movement. in addition, we deploy a multi-task adversarial training scheme in the context of video generation to improve both photo-realism and the accuracy for lip synchronization. finally, based on the phoneme distribution information extracted from the audio clip, we develop a sample selection method that effectively reduces the size of the training dataset without sacrificing the quality of the generated video. extensive experiments on both controlled and uncontrolled datasets demonstrate the superiority of the proposed approach in terms of visual quality, lip sync accuracy, and smooth transition of lip and facial movement, as compared to the state-of-the-art."
