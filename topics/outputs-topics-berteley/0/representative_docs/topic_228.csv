doc
"holistic 3d scene understanding from a single image with implicit representation. we present a new pipeline for holistic 3d scene understanding from a single image, which could predict object shapes, object poses, and scene layout. as it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. we propose to utilize the latest deep implicit representation to solve this challenge. we not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine the 3d object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. a novel physical violation loss is also proposed to avoid incorrect context between objects. extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3d object detection."
"category level object pose estimation via neural analysis-by-synthesis. many object pose estimation algorithms rely on the analysis-by-synthesis framework which requires explicit representations of individual object instances. in this paper we combine a gradient-based fitting procedure with a parametric neural image synthesis module that is capable of implicitly representing the appearance, shape and pose of entire object categories, thus rendering the need for explicit cad models per object instance unnecessary. the image synthesis network is designed to efficiently span the pose configuration space so that model capacity can be used to capture the shape and local appearance (i.e., texture) variations jointly. at inference time the synthesized images are compared to the target via an appearance based loss and the error signal is backpropagated through the network to the input parameters. keeping the network parameters fixed, this allows for iterative optimization of the object pose, shape and appearance in a joint manner and we experimentally show that the method can recover orientation of objects with high accuracy from 2d images alone. when provided with depth measurements, to overcome scale ambiguities, the method can accurately recover the full 6dof pose successfully."
"gs-pose: category-level object pose estimation via geometric and semantic correspondence. category-level pose estimation is a challenging task with many potential applications in computer vision and robotics. recently, deep-learning-based approaches have made great progress, but are typically hindered by the need for large datasets of either pose-labelled real images or carefully tuned photorealistic simulators. this can be avoided by using only geometry inputs such as depth images to reduce the domain-gap but these approaches suffer from a lack of semantic information, which can be vital in the pose estimation problem. to resolve this conflict, we propose to utilize both geometric and semantic features obtained from a pre-trained foundation model.our approach projects 2d features from this foundation model into 3d for a single object model per category, and then performs matching against this for new single view observations of unseen object instances with a trained matching network. this requires significantly less data to train than prior methods since the semantic features are robust to object texture and appearance. we demonstrate this with a rich evaluation, showing improved performance over prior methods with a fraction of the data required."
