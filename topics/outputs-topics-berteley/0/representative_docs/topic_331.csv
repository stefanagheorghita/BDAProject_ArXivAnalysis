doc
"oimnet++: prototypical normalization and localization-aware learning for person search. we address the task of person search, that is, localizing and re-identifying query persons from a set of raw scene images. recent approaches are typically built upon oimnet, a pioneer work on person search, that learns joint person representations for performing both detection and person re-identification (reid) tasks. to obtain the representations, they extract features from pedestrian proposals, and then project them on a unit hypersphere with l2 normalization. these methods also incorporate all positive proposals, that sufficiently overlap with the ground truth, equally to learn person representations for reid. we have found that 1) the l2 normalization without considering feature distributions degenerates the discriminative power of person representations, and 2) positive proposals often also depict background clutter and person overlaps, which could encode noisy features to person representations. in this paper, we introduce oimnet++ that addresses the aforementioned limitations. to this end, we introduce a novel normalization layer, dubbed protonorm, that calibrates features from pedestrian proposals, while considering a long-tail distribution of person ids, enabling l2 normalized person representations to be discriminative. we also propose a localization-aware feature learning scheme that encourages better-aligned proposals to contribute more in learning discriminative representations. experimental results and analysis on standard person search benchmarks demonstrate the effectiveness of oimnet++."
"a unified deep semantic expansion framework for domain-generalized person re-identification. supervised person re-identification (person reid) methods have achieved excellent performance when training and testing within one camera network. however, they usually suffer from considerable performance degradation when applied to different camera systems. in recent years, many domain adaptation person reid methods have been proposed, achieving impressive performance without requiring labeled data from the target domain. however, these approaches still need the unlabeled data of the target domain during the training process, making them impractical in many real-world scenarios. our work focuses on the more practical domain generalized person re-identification (dg-reid) problem. given one or more source domains, it aims to learn a generalized model that can be applied to unseen target domains. one promising research direction in dg-reid is the use of implicit deep semantic feature expansion, and our previous method, domain embedding expansion (dex), is one such example that achieves powerful results in dg-reid. however, in this work we show that dex and other similar implicit deep semantic feature expansion methods, due to limitations in their proposed loss function, fail to reach their full potential on large evaluation benchmarks as they have a tendency to saturate too early. leveraging on this analysis, we propose unified deep semantic expansion, our novel framework that unifies implicit and explicit semantic feature expansion techniques in a single framework to mitigate this early over-fitting and achieve a new state-of-the-art (sota) in all dg-reid benchmarks. further, we apply our method on more general image retrieval tasks, also surpassing the current sota in all of these benchmarks by wide margins."
"diverse deep feature ensemble learning for omni-domain generalized person re-identification. person re-identification (person reid) has progressed to a level where single-domain supervised person reid performance has saturated. however, such methods experience a significant drop in performance when trained and tested across different datasets, motivating the development of domain generalization techniques. however, our research reveals that domain generalization methods significantly underperform single-domain supervised methods on single dataset benchmarks. an ideal person reid method should be effective regardless of the number of domains involved, and when test domain data is available for training it should perform as well as state-of-the-art (sota) fully supervised methods. this is a paradigm that we call omni-domain generalization person reid (odg-reid). we propose a way to achieve odg-reid by creating deep feature diversity with self-ensembles. our method, diverse deep feature ensemble learning (d2fel), deploys unique instance normalization patterns that generate multiple diverse views and recombines these views into a compact encoding. to the best of our knowledge, our work is one of few to consider omni-domain generalization in person reid, and we advance the study of applying feature ensembles in person reid. d2fel significantly improves and matches the sota performance for major domain generalization and single-domain supervised benchmarks."
