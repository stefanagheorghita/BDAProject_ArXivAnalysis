doc
"do we still need clinical language models?. although recent advances in scaling large language models (llms) have resulted in improvements on many nlp tasks, it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as clinical text. recent results have suggested that llms encode a surprising amount of medical knowledge. this raises an important question regarding the utility of smaller domain-specific language models. with the success of general-domain llms, is there still a need for specialized clinical models? to investigate this question, we conduct an extensive empirical analysis of 12 language models, ranging from 220m to 175b parameters, measuring their performance on 3 different clinical tasks that test their ability to parse and reason over electronic health records. as part of our experiments, we train t5-base and t5-large models from scratch on clinical notes from mimic iii and iv to directly investigate the efficiency of clinical tokens. we show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data. further, we find that pretraining on clinical tokens allows for smaller, more parameter-efficient models that either match or outperform much larger language models trained on general text. we release the code and the models used under the physionet credentialed health data license and data use agreement."
"hierarchical annotation for building a suite of clinical natural language processing tasks: progress note understanding. applying methods in natural language processing on electronic health records (ehr) data is a growing field. existing corpus and annotation focus on modeling textual features and relation prediction. however, there is a paucity of annotated corpus built to model clinical diagnostic thinking, a process involving text understanding, domain knowledge abstraction and reasoning. this work introduces a hierarchical annotation schema with three stages to address clinical text understanding, clinical reasoning, and summarization. we created an annotated corpus based on an extensive collection of publicly available daily progress notes, a type of ehr documentation that is collected in time series in a problem-oriented format. the conventional format for a progress note follows a subjective, objective, assessment and plan heading (soap). we also define a new suite of tasks, progress note understanding, with three tasks utilizing the three annotation stages. the novel suite of tasks was designed to train and evaluate future nlp models for clinical text understanding, clinical knowledge representation, inference, and summarization."
"large language models are few-shot clinical information extractors. a long-running goal of the clinical nlp community is the extraction of important variables trapped in clinical notes. however, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. in this work, we show that large language models, such as instructgpt, perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of nlp tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the casi dataset for new tasks. on the clinical extraction tasks we studied, the gpt-3 systems significantly outperform existing zero- and few-shot baselines."
