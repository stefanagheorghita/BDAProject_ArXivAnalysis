doc
relu neural network linear layer biased multi index model neural network operate overparameterized regime far parameter training sample allow training datum fit perfectly train network effectively learn interpolate function property interpolant affect prediction network new sample manuscript explore property function learn neural network depth great layer framework consider family network vary depth capacity different representation cost representation cost function induce neural network architecture minimum sum square weight need network represent function reflect function space bias associate architecture result add additional linear layer input shallow relu network yield representation cost favor function low mixed variation limit variation direction orthogonal low dimensional subspace approximate multi index model bias occur minimize sum square weight linear layer equivalent minimize low rank promote schatten quasi norm single virtual weight matrix experiment confirm behavior standard network training regime additionally linear layer improve generalization learn network align true latent low dimensional linear subspace datum generate multi index model
algorithmic construction deep relu network difficult describe mathematical term neural network train datum represent hand grow mathematical understanding neural network principle capable represent feedforward neural network relu activation function represent continuous piecewise linear function approximate study expressivity address question one contribute available answer perspective neural network algorithm analogy neural network program constructively train datum interesting example sort algorithm explicitly construct neural network sort input exactly approximately sense optimal computational complexity input dimension large construct network billion parameter construct analyze example exist new find example neural network algorithm typically recursive parallel compare conventional algorithm relu network restrict have continuous depth recursion limit depth network deep network have superior property shallow one
convergence theory deep learning parameterization deep neural network dnn demonstrate dominate performance field alexnet network practice go wide deep theoretical long line work focus training neural network hide layer theory multi layer network remain largely unsettled work prove stochastic gradient descent sgd find training objective dnn assumption input non degenerate network parameterized mean network width sufficiently large number layer number sample key technique derive sufficiently large neighborhood random initialization optimization landscape convex semi smooth relu activation imply equivalence parameterize neural network neural tangent kernel ntk finite polynomial width setting concrete example start randomly initialize weight prove sgd attain training accuracy classification task minimize regression loss linear convergence speed run time polynomial n theory apply widely non smooth relu activation smooth possibly non convex loss function term network architecture theory apply fully connect neural network convolutional neural network cnn residual neural network resnet
