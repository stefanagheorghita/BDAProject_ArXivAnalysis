doc
control forgetting test time datum continual learning foundational vision language model show impressive performance downstream task press need update model later new task domain available ongoing continual learning cl research provide technique overcome catastrophic forgetting previous information new knowledge acquire date cl technique focus supervised training session result significant forgetting yield inferior performance prior model zero shot performance work argue test time datum hold great information leverage self supervise manner refresh model memory previous learn task greatly reduce forget extra labelling cost study unsupervised datum employ online improve model performance prior task encounter representative sample propose simple effective student teacher model gradient base sparse parameter update significant performance improvement reduction forgetting alleviate role offline episodic memory experience replay buffer
contrastive continual learning importance sampling prototype instance relation distillation recently high quality representation contrastive learning method rehearsal base contrastive continual learning propose explore continually learn transferable representation embedding avoid catastrophic forgetting issue traditional continual setting base framework propose contrastive continual learning importance sampling cclis preserve knowledge recover previous data distribution new strategy replay buffer selection rbs minimize estimate variance save hard negative sample representation learning high quality furthermore present prototype instance relation distillation prd loss technique design maintain relationship prototype sample representation self distillation process experiment standard continual learning benchmark reveal method notably outperform exist baseline term knowledge preservation effectively counteract catastrophic forgetting online contexts code available
continual learning bayesian model base fix pre train feature extractor deep learning show human level performance application current deep learning model characterise catastrophic forgetting old knowledge learn new class pose challenge particularly intelligent diagnosis system initially train datum limited number disease available case update intelligent system datum new disease inevitably downgrade performance previously learn disease inspire process learn new knowledge human brain propose bayesian generative model continual learning build fix pre train feature extractor model knowledge old class compactly represent collection statistical distribution gaussian mixture model naturally keep forget continual learning time unlike exist class incremental learning method propose approach sensitive continual learning process additionally apply data incremental learning scenario experiment multiple medical natural image classification task show propose approach outperform state art approach image old class continual learning new class
