doc
automatic pruning quantize neural network neural network quantization pruning technique commonly reduce computational complexity memory footprint model deployment exist pruning strategy operate precision directly apply discrete parameter distribution quantization contrast study combination technique achieve network compression particular propose effective pruning strategy select redundant low precision filter furthermore leverage bayesian optimization efficiently determine pruning ratio layer conduct extensive experiment imagenet architecture precision particular imagenet prune model size binarize neural network quantization achieve classification accuracy model mb bit dorefa net mb
ps qs quantization aware pruning efficient low latency neural network inference efficient machine learn implementation optimize inference hardware wide range benefit depend application low inference latency high datum throughput reduce energy consumption popular technique reduce computation neural network prune remove insignificant synapsis quantization reduce precision calculation work explore interplay pruning quantization training neural network ultra low latency application target high energy physics use case technique develop study potential application domain study configuration prune quantization aware training term quantization aware pruning effect technique like regularization batch normalization different pruning scheme performance computational complexity information content metric find quantization aware pruning yield computationally efficient model pruning quantization task quantization aware pruning typically perform similar well term computational efficiency compare neural architecture search technique like bayesian optimization surprisingly network different training configuration similar performance benchmark application information content network vary significantly affect generalizability
learning filter level heterogeneous compression convolutional neural network recently deep learning de facto standard machine learn convolutional neural network cnn demonstrate spectacular success wide variety task cnn typically demanding computationally inference time way alleviate burden certain hardware platform quantization rely use low precision arithmetic representation weight activation popular method pruning number filter layer mainstream deep learning method train neural network weight keep network architecture fix emerge neural architecture search nas technique amenable training paper formulate optimal arithmetic bit length allocation neural network prune nas problem search configuration satisfy computational complexity budget maximize accuracy use differentiable search method base continuous relaxation search space propose liu et al grid search heterogeneous quantize network suffer high variance render benefit search questionable pruning improvement homogeneous case possible challenge find configuration propose method code publicly available
