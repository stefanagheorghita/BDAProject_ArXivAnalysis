doc
decomposable norm minimization proximal gradient homotopy algorithm study convergence rate proximal gradient homotopy algorithm apply norm regularize linear square problem general class norm homotopy algorithm reduce regularization parameter series step use proximal gradient algorithm solve problem step proximal gradient algorithm linear rate convergence give objective function strongly convex gradient smooth component objective function lipschitz continuous application objective function type problem strongly convex especially problem high dimensional regularizer choose induce sparsity low dimensionality linear sample matrix satisfy certain assumption regularize norm decomposable proximal gradient homotopy algorithm converge rate objective function strongly convex result generalize result linear convergence homotopy algorithm square problem numerical experiment present support theoretical convergence rate analysis
proximal stochastic gradient method progressive variance reduction consider problem minimize sum convex function average large number smooth component function general convex function admit simple proximal mapping assume objective function strongly convex problem arise machine learning know regularize empirical risk minimization propose analyze new proximal stochastic gradient method use multi stage scheme progressively reduce variance stochastic gradient iteration algorithm similar cost classical stochastic gradient method incremental gradient method expect objective value converge optimum geometric rate overall complexity method low proximal gradient method standard proximal stochastic gradient method
stochastic gradient free descent paper propose stochastic gradient free method accelerate method momentum solve stochastic optimization problem method rely stochastic direction stochastic gradient analyze convergence behavior method mean variance framework provide theoretical analysis inclusion momentum stochastic setting reveal momentum term add deviation order control variance order iteration show employ decaying stepsize stochastic gradient free method maintain sublinear convergence rate accelerated method momentum achieve convergence rate probability strongly convex objective lipschitz gradient method converge solution zero expect gradient norm objective function nonconvex twice differentiable bound
