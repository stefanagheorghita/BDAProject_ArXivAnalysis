doc
bridge language gap large language model inference time cross lingual intervention large language model llm show remarkable capability natural language processing exhibit significant performance gap different language exist approach address disparity rely pretraine fine tuning resource intensive overcome limitation incur significant cost propose inference time cross lingual intervention incline novel framework enhance llm performance low perform source language align internal representation high perform target language inference incline initially learn alignment matrix parallel sentence source target language square optimization apply matrix inference transform low perform language representation high perform language space extensive experiment benchmark llm demonstrate incline significantly improve performance diverse task language compare recent strong baseline analysis demonstrate incline highly cost effective applicable wide range application addition release code foster research line
improve neural machine translation pre train representation monolingual datum demonstrate helpful improve translation quality neural machine translation nmt current method stay usage word level knowledge generate synthetic parallel datum extract information word embed contrast power sentence level contextual knowledge complex diverse play important role natural language generation fully exploit paper propose novel structure leverage monolingual datum acquire sentence level contextual representation design framework integrate source target sentence level representation nmt model improve translation quality experimental result chinese english german english machine translation task propose model achieve improvement strong transformer baseline experiment english turkish demonstrate effectiveness approach low resource scenario
neural resource language translation comparative evaluation approach resource language minimal digital representation pose unique challenge machine translation mt unlike low resource language rely limited existent corpora resource language few sentence available training work explore problem resource translation distinct workflow fine tuning translation specific model context learning large language model llm chain reasoning prompting direct prompting reasoning owens valley paiute case study demonstrate resource translation demand fundamentally different approach low resource scenario traditional approach machine translation work low resource language fail empirical result reveal traditional approach fail context learn capability general purpose large language model enable resource language translation outperform low resource translation approach rival human translation bleu specifically chain reasoning prompting outperform method large corpora direct prompt exhibit advantage small dataset approach language agnostic potential generalize translation task wide variety resource language expert input finding establish resource translation distinct paradigm require innovative solution provide practical theoretical insight language preservation
