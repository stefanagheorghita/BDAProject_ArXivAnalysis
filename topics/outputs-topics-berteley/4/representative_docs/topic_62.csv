doc
subspace iteration randomization singular value problem classical problem matrix computation efficient reliable approximation give matrix matrix low rank truncated singular value decomposition svd know provide good approximation give fix rank svd know costly compute different approach literature compute low rank approximation randomized algorithm attract researcher recent attention surprising reliability computational efficiency different application area typically algorithm show compute high probability low rank approximation constant factor optimal know perform well practical situation paper present novel error analysis consider randomize algorithm subspace iteration framework high probability highly accurate low rank approximation singular value compute quickly matrix rapidly decay singular value matrix appear frequently diverse application area datum analysis fast structured matrix computation fast direct method large sparse linear system equation drive motivation randomized method furthermore low rank approximation compute randomize algorithm actually rank reveal approximation special case approximation correctly estimate matrix norm high probability numerical experiment support conclusion
tensor completion hierarchical tensor representation compress sensing extend recovery sparse vector undersampled measurement efficient algorithm recovery matrix low rank incomplete information consider extension reconstruction tensor low multi linear rank recently introduce hierarchical tensor format small number measurement hierarchical tensor flexible generalization know tucker representation advantage number degree freedom low rank tensor scale exponentially order tensor correspond tensor decomposition compute efficiently successive application matrix singular value decomposition important property singular value decomposition extend matrix tensor case result major computational theoretical difficulty design analyze algorithm low rank tensor recovery instance canonical analogue tensor nuclear norm np hard compute general stark contrast matrix case book chapter consider version iterative hard thresholde scheme adapt hierarchical tensor format variant build method riemannian optimization use retraction mapping tangent space manifold low rank tensor manifold provide partial convergence result base tensor version restrict isometry property trip measurement map estimate number measurement provide ensure trip give tensor rank high probability gaussian measurement map
recovery low rank plus compress sparse matrix application unveil traffic anomaly give superposition low rank matrix plus product know fat compression matrix time sparse matrix goal paper establish deterministic condition exact recovery low rank sparse component possible fundamental identifiability issue arise traffic anomaly detection backbone network subsume compress sensing timely low rank plus sparse matrix recovery task encounter matrix decomposition problem leverage ability nuclear norm recover sparse low rank matrix convex program formulate estimate unknown analysis simulation confirm say convex program recover unknown sufficiently low rank sparse component compression matrix possess isometry property restrict operate sparse vector low rank sparse compression matrix draw certain random ensemble establish exact recovery possible high probability order algorithm develop solve nonsmooth convex optimization problem provable iteration complexity guarantee insightful test synthetic real network datum corroborate effectiveness novel approach unveil traffic anomaly flow time ability outperform exist alternative
