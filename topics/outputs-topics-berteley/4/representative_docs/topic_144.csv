doc
pathseeker explore llm security vulnerability reinforcement learning base jailbreak approach recent year large language model llm gain widespread use raise concern security traditional jailbreak attack rely model internal information limitation explore unsafe behavior victim model limit reduce general applicability paper introduce pathseeker novel black box jailbreak method inspire game rat escape maze think llm unique security maze attacker attempt find exit learning receive feedback accumulate experience compromise target llm security defence approach leverage multi agent reinforcement learning small model collaborate guide main llm perform mutation operation achieve attack objective progressively modify input base model feedback system induce rich harmful response manual attempt perform jailbreak attack find vocabulary response target model gradually rich eventually produce harmful response base observation introduce reward mechanism exploit expansion vocabulary richness llm response weaken security constraint method outperform state art attack technique test commercial open source llm achieve high attack success rate especially strongly align commercial model like mini air strong safety alignment study aim improve understanding llm security vulnerability hope sturdy contribute development robust defense
play language game llm lead jailbreake advent large language model llm spur development numerous jailbreak technique aim circumvent security defense malicious attack effective jailbreak approach identify domain safety generalization fail phenomenon know mismatch generalization paper introduce novel jailbreak method base mismatch generalization natural language game custom language game effectively bypass safety mechanism llm kind different variant make hard defend lead high attack rate natural language game involve use synthetic linguistic construct action intertwine construct ubbi dubbi language build phenomenon propose custom language game method engage llm variety custom rule successfully execute jailbreak attack multiple llm platform extensive experiment demonstrate effectiveness method achieve success rate mini sonnet furthermore investigate generalizability safety alignment fine tuned custom language game achieve safety alignment dataset find interact language game fine tune model fail identify harmful content finding indicate safety alignment knowledge embed llm fail generalize different linguistic format open new avenue future research area
alignment jailbreak work explain llm safety intermediate hide state large language model llm rely safety alignment avoid respond malicious user input unfortunately jailbreak circumvent safety guardrail result llm generate harmful content raise concern llm safety language model intensive parameter regard black box mechanism alignment jailbreak challenge elucidate paper employ weak classifier explain llm safety intermediate hide state confirm llm learn ethical concept pre training alignment identify malicious normal input early layer alignment actually associate early concept emotion guess middle layer refine specific reject token safe generation jailbreak disturb transformation early unethical classification negative emotion conduct experiment model model family prove conclusion overall paper indicate intrinsical mechanism llm safety jailbreak circumvent safety guardrail offer new perspective llm safety reduce concern code available
