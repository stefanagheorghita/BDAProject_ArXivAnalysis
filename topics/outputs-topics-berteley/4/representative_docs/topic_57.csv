doc
proximal augmented lagrangian method nonsmooth composite optimization study class optimization problem objective function give sum differentiable possibly nonconvex component nondifferentiable convex regularization term introduce auxiliary variable separate objective function component utilize moreau envelope regularization term derive proximal augment lagrangian continuously differentiable function obtain constrain augmented lagrangian manifold correspond explicit minimization variable nonsmooth term continuous differentiability function respect primal dual variable allow leverage method multiplier mm compute optimal primal dual pair solve sequence differentiable problem mm algorithm applicable broad class problem proximal gradient method strong convergence guarantee refined step size update rule alternate direction method multiplier feature attractive option solve structured optimal control problem develop algorithm base primal descent dual ascent gradient method prove global exponential asymptotic stability differentiable component objective function strongly convex regularization term convex finally identify class problem primal dual gradient flow dynamic convenient distribute implementation compare contrast framework exist approach
random coordinate descent algorithm optimization problem composite objective function linear couple constraint paper propose variant random coordinate descent method solve linearly constrained convex optimization problem composite objective function smooth objective function lipschitz continuous gradient prove method obtain solution iteration number block class problem cheap coordinate derivative new method fast method base gradient information analysis rate convergence probability provide strongly convex function method converge linearly extensive numerical test confirm large problem method numerically efficient method base gradient information
variance reduction fast non convex optimization consider fundamental problem non convex optimization efficiently reach stationary point contrast convex case long history basic problem know theoretical result order non convex optimization remain gradient descent converge iteration smooth objective stochastic gradient descent converge iteration objective sum smooth function provide improvement line research result base variance reduction trick recently introduce convex optimization brand new analysis variance reduction suitable non convex optimization objective sum smooth function order minibatch stochastic method converge rate fast gradient descent demonstrate effectiveness method empirical risk minimization non convex loss function train neural net
