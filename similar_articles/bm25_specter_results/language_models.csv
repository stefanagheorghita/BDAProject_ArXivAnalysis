query_type,query_value,rank,doc_id,specter_score,title,categories,abstract
text,large language models transformers pretraining,1,2410.24159,0.8003554344177246,GPT or BERT: why not both?,cs.CL,"  We present a simple way to merge masked language modeling with causal
language modeling. This hybrid training objective results in a model that
combines the strengths of both modeling paradigms within a single transformer
stack: GPT-BERT can be transparently used like any standard causal or masked
language model. We test the pretraining process that enables this flexible
behavior on the BabyLM Challenge 2024. The results show that the hybrid
pretraining outperforms masked-only or causal-only models. We openly release
the models, training corpora and code.
"
text,large language models transformers pretraining,2,1910.03771,0.8000511527061462,HuggingFace's Transformers: State-of-the-art Natural Language Processing,cs.CL,"  Recent progress in natural language processing has been driven by advances in
both model architecture and model pretraining. Transformer architectures have
facilitated building higher-capacity models and pretraining has made it
possible to effectively utilize this capacity for a wide variety of tasks.
\textit{Transformers} is an open-source library with the goal of opening up
these advances to the wider machine learning community. The library consists of
carefully engineered state-of-the art Transformer architectures under a unified
API. Backing this library is a curated collection of pretrained models made by
and available for the community. \textit{Transformers} is designed to be
extensible by researchers, simple for practitioners, and fast and robust in
industrial deployments. The library is available at
\url{https://github.com/huggingface/transformers}.
"
text,large language models transformers pretraining,3,2305.14857,0.7990422248840332,"BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual
  Transfer",cs.CL,"  Despite remarkable advancements in few-shot generalization in natural
language processing, most models are developed and evaluated primarily in
English. To facilitate research on few-shot cross-lingual transfer, we
introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across
54 languages in a sequence-to-sequence format and provides a fixed set of
few-shot examples and instructions. BUFFET is designed to establish a rigorous
and equitable evaluation framework for few-shot cross-lingual transfer across a
broad range of tasks and languages. Using BUFFET, we perform thorough
evaluations of state-of-the-art multilingual large language models with
different transfer methods, namely in-context learning and fine-tuning. Our
findings reveal significant room for improvement in few-shot in-context
cross-lingual transfer. In particular, ChatGPT with in-context learning often
performs worse than much smaller mT5-base models fine-tuned on English task
data and few-shot in-language examples. Our analysis suggests various avenues
for future research in few-shot cross-lingual transfer, such as improved
pretraining, understanding, and future evaluations.
"
text,large language models transformers pretraining,4,2402.19204,0.793854832649231,"PeLLE: Encoder-based language models for Brazilian Portuguese based on
  open data",cs.CL,"  In this paper we present PeLLE, a family of large language models based on
the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open
data from the Carolina corpus. Aiming at reproducible results, we describe
details of the pretraining of the models. We also evaluate PeLLE models against
a set of existing multilingual and PT-BR refined pretrained Transformer-based
LLM encoders, contrasting performance of large versus smaller-but-curated
pretrained models in several downstream tasks. We conclude that several tasks
perform better with larger models, but some tasks benefit from
smaller-but-curated data in its pretraining.
"
text,large language models transformers pretraining,5,2308.16336,0.793576180934906,"ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language
  Understanding",cs.CL cs.LG,"  We present ToddlerBERTa, a BabyBERTa-like language model, exploring its
capabilities through five different models with varied hyperparameters.
Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the
BabyLM challenge, we find that smaller models can excel in specific tasks,
while larger models perform well with substantial data. Despite training on a
smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling
the state-of-the-art RoBERTa-base. The model showcases robust language
understanding, even with single-sentence pretraining, and competes with
baselines that leverage broader contextual information. Our work provides
insights into hyperparameter choices, and data utilization, contributing to the
advancement of language models.
"
text,large language models transformers pretraining,6,2011.04784,0.7934012413024902,EstBERT: A Pretrained Language-Specific BERT for Estonian,cs.CL,"  This paper presents EstBERT, a large pretrained transformer-based
language-specific BERT model for Estonian. Recent work has evaluated
multilingual BERT models on Estonian tasks and found them to outperform the
baselines. Still, based on existing studies on other languages, a
language-specific BERT model is expected to improve over the multilingual ones.
We first describe the EstBERT pretraining process and then present the results
of the models based on finetuned EstBERT for multiple NLP tasks, including POS
and morphological tagging, named entity recognition and text classification.
The evaluation results show that the models based on EstBERT outperform
multilingual BERT models on five tasks out of six, providing further evidence
towards a view that training language-specific BERT models are still useful,
even when multilingual models are available.
"
text,large language models transformers pretraining,7,2311.02265,0.7919847965240479,Not all layers are equally as important: Every Layer Counts BERT,cs.CL,"  This paper introduces a novel modification of the transformer architecture,
tailored for the data-efficient pretraining of language models. This aspect is
evaluated by participating in the BabyLM challenge, where our solution won both
the strict and strict-small tracks. Our approach allows each transformer layer
to select which outputs of previous layers to process. The empirical results
verify the potential of this simple modification and show that not all layers
are equally as important.
"
text,large language models transformers pretraining,8,2412.04092,0.7907675504684448,GEITje 7B Ultra: A Conversational Model for Dutch,cs.CL,"  Language models have rapidly evolved, predominantly focusing on English while
often neglecting extensive pretraining in other languages. This approach has
required initiatives to adapt powerful, English-centric models to other
linguistic contexts through finetuning. For Dutch, such a recent endeavour is
``GEITje'' a model originally derived from the English-based Mistral 7B.
Building on this fundamental work, the current research extends the
capabilities of GEITje by supervised finetuning on newly created high-quality
synthetic conversational datasets, along with an additional preference
alignment procedure on a synthetic feedback dataset. Both the developed models
and the created datasets are openly available.
"
text,large language models transformers pretraining,9,2102.12982,0.7890999913215637,"A Primer on Contrastive Pretraining in Language Processing: Methods,
  Lessons Learned and Perspectives",cs.CL cs.AI cs.CV,"  Modern natural language processing (NLP) methods employ self-supervised
pretraining objectives such as masked language modeling to boost the
performance of various application tasks. These pretraining methods are
frequently extended with recurrence, adversarial or linguistic property
masking, and more recently with contrastive learning objectives. Contrastive
self-supervised training objectives enabled recent successes in image
representation pretraining by learning to contrast input-input pairs of
augmented images as either similar or dissimilar. However, in NLP, automated
creation of text input augmentations is still very challenging because a single
token can invert the meaning of a sentence. For this reason, some contrastive
NLP pretraining methods contrast over input-label pairs, rather than over
input-input pairs, using methods from Metric Learning and Energy Based Models.
In this survey, we summarize recent self-supervised and supervised contrastive
NLP pretraining methods and describe where they are used to improve language
modeling, few or zero-shot learning, pretraining data-efficiency and specific
NLP end-tasks. We introduce key contrastive learning concepts with lessons
learned from prior research and structure works by applications and cross-field
relations. Finally, we point to open challenges and future directions for
contrastive NLP to encourage bringing contrastive NLP pretraining closer to
recent successes in image representation pretraining.
"
text,large language models transformers pretraining,10,2006.13979,0.7879651784896851,"Unsupervised Cross-lingual Representation Learning for Speech
  Recognition",cs.CL cs.LG cs.SD eess.AS,"  This paper presents XLSR which learns cross-lingual speech representations by
pretraining a single model from the raw waveform of speech in multiple
languages. We build on wav2vec 2.0 which is trained by solving a contrastive
task over masked latent speech representations and jointly learns a
quantization of the latents shared across languages. The resulting model is
fine-tuned on labeled data and experiments show that cross-lingual pretraining
significantly outperforms monolingual pretraining. On the CommonVoice
benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared
to the best known results. On BABEL, our approach improves word error rate by
16% relative compared to a comparable system. Our approach enables a single
multilingual speech recognition model which is competitive to strong individual
models. Analysis shows that the latent discrete speech representations are
shared across languages with increased sharing for related languages. We hope
to catalyze research in low-resource speech understanding by releasing XLSR-53,
a large model pretrained in 53 languages.
"
text,large language models transformers pretraining,11,2412.17933,0.7854419946670532,BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism,cs.CL cs.AI,"We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its duel scoring system is grounded in statistical significance theory and uses aggregation across tasks inspired by social preference theory. Our benchmark encompasses 50 challenging tasks, with corresponding test datasets, primarily in native Czech, with 14 newly collected ones. These tasks span 8 categories and cover diverse domains, including historical Czech news, essays from pupils or language learners, and spoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the largest publicly available clean Czech language corpus, and use it for (i) contamination analysis and (ii) continuous pretraining of the first Czech-centric 7B language model with Czech-specific tokenization. We use our model as a baseline for comparison with publicly available multilingual models. Lastly, we release and maintain a leaderboard with existing 50 model submissions, where new model submissions can be made at https://huggingface.co/spaces/CZLC/BenCzechMark."
text,large language models transformers pretraining,12,2404.07965,0.785344123840332,Rho-1: Not All Tokens Are What You Need,cs.CL cs.AI,"  Previous language model pre-training methods have uniformly applied a
next-token prediction loss to all training tokens. Challenging this norm, we
posit that ""9l training"". Our initial analysis examines token-level training
dynamics of language model, revealing distinct loss patterns for different
tokens. Leveraging these insights, we introduce a new language model called
Rho-1. Unlike traditional LMs that learn to predict every next token in a
corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively
trains on useful tokens that aligned with the desired distribution. This
approach involves scoring pretraining tokens using a reference model, and then
training the language model with a focused loss on tokens with higher scores.
When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute
improvement in few-shot accuracy of up to 30% in 9 math tasks. After
fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and
51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the
pretraining tokens. Furthermore, when continual pretraining on 80B general
tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks,
increasing both efficiency and performance of the language model pre-training.
"
text,large language models transformers pretraining,13,1812.10860,0.7849318385124207,"Can You Tell Me How to Get Past Sesame Street? Sentence-Level
  Pretraining Beyond Language Modeling",cs.CL,"  Natural language understanding has recently seen a surge of progress with the
use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et
al., 2019) which are pretrained on variants of language modeling. We conduct
the first large-scale systematic study of candidate pretraining tasks,
comparing 19 different tasks both as alternatives and complements to language
modeling. Our primary results support the use language modeling, especially
when combined with pretraining on additional labeled-data tasks. However, our
results are mixed across pretraining tasks and show some concerning trends: In
ELMo's pretrain-then-freeze paradigm, random baselines are worryingly strong
and results vary strikingly across target tasks. In addition, fine-tuning BERT
on an intermediate task often negatively impacts downstream transfer. In a more
positive trend, we see modest gains from multitask training, suggesting the
development of more sophisticated multitask and transfer learning techniques as
an avenue for further research.
"
text,large language models transformers pretraining,14,2410.09817,0.7842046022415161,Reverse Modeling in Large Language Models,cs.CL,"  Humans are accustomed to reading and writing in a forward manner, and this
natural bias extends to text understanding in auto-regressive large language
models (LLMs). This paper investigates whether LLMs, like humans, struggle with
reverse modeling, specifically with reversed text inputs. We found that
publicly available pre-trained LLMs cannot understand such inputs. However,
LLMs trained from scratch with both forward and reverse texts can understand
them equally well during inference across multiple languages. Our case study
shows that different-content texts result in different losses if input (to
LLMs) in different directions -- some get lower losses for forward while some
for reverse. This leads us to a simple and nice solution for data selection
based on the loss differences between forward and reverse directions. Using our
selected data in continued pretraining can boost LLMs' performance by a large
margin across different language understanding benchmarks.
"
text,large language models transformers pretraining,15,2101.10717,0.7827293872833252,"Combining Deep Generative Models and Multi-lingual Pretraining for
  Semi-supervised Document Classification",cs.CL,"  Semi-supervised learning through deep generative models and multi-lingual
pretraining techniques have orchestrated tremendous success across different
areas of NLP. Nonetheless, their development has happened in isolation, while
the combination of both could potentially be effective for tackling
task-specific labelled data shortage. To bridge this gap, we combine
semi-supervised deep generative models and multi-lingual pretraining to form a
pipeline for document classification task. Compared to strong supervised
learning baselines, our semi-supervised classification framework is highly
competitive and outperforms the state-of-the-art counterparts in low-resource
settings across several languages.
"
text,large language models transformers pretraining,16,2410.06898,0.7822329998016357,Generative Model for Less-Resourced Language with 1 billion parameters,cs.CL,"  Large language models (LLMs) are a basic infrastructure for modern natural
language processing. Many commercial and open-source LLMs exist for English,
e.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on
mostly English texts, their fluency and knowledge of low-resource languages and
societies are superficial. We present the development of large generative
language models for a less-resourced language. GaMS 1B - Generative Model for
Slovene with 1 billion parameters was created by continuing pretraining of the
existing English OPT model. We developed a new tokenizer adapted to Slovene,
Croatian, and English languages and used embedding initialization methods FOCUS
and WECHSEL to transfer the embeddings from the English OPT model. We evaluate
our models on several classification datasets from the Slovene suite of
benchmarks and generative sentence simplification task SENTA. We only used a
few-shot in-context learning of our models, which are not yet
instruction-tuned. For classification tasks, in this mode, the generative
models lag behind the existing Slovene BERT-type models fine-tuned for specific
tasks. On a sentence simplification task, the GaMS models achieve comparable or
better performance than the GPT-3.5-Turbo model.
"
text,large language models transformers pretraining,17,2409.14705,0.7812382578849792,Target-Aware Language Modeling via Granular Data Sampling,cs.CL cs.AI,"  Language model pretraining generally targets a broad range of use cases and
incorporates data from diverse sources. However, there are instances where we
desire a model that excels in specific areas without markedly compromising
performance in other areas. A cost-effective and straightforward approach is
sampling with low-dimensional data features, which allows to select large-scale
pretraining data for domain-specific use cases. In this work, we revisit
importance sampling with n-gram features consisting of multi-granular tokens,
which strikes a good balance between sentence compression and representation
capabilities. We observed the sampled data to have a high correlation with the
target downstream task performance while preserving its effectiveness on other
tasks. This leads to the proposed data sampling paradigm where language models
can be pretrained more efficiently on selected documents. On eight benchmarks
we demonstrate with $\sim$1% of the data, pretrained models perform on par with
the full RefinedWeb data and outperform randomly selected samples for model
sizes ranging from 125M to 1.5B.
"
text,large language models transformers pretraining,18,2110.08534,0.7807649374008179,"Lifelong Pretraining: Continually Adapting Language Models to Emerging
  Corpora",cs.CL,"  Pretrained language models (PTLMs) are typically learned over a large, static
corpus and further fine-tuned for various downstream tasks. However, when
deployed in the real world, a PTLM-based model must deal with data
distributions that deviate from what the PTLM was initially trained on. In this
paper, we study a lifelong language model pretraining challenge where a PTLM is
continually updated so as to adapt to emerging data. Over a domain-incremental
research paper stream and a chronologically-ordered tweet stream, we
incrementally pretrain a PTLM with different continual learning algorithms, and
keep track of the downstream task performance (after fine-tuning). We evaluate
PTLM's ability to adapt to new corpora while retaining learned knowledge in
earlier corpora. Our experiments show distillation-based approaches to be most
effective in retaining downstream performance in earlier domains. The
algorithms also improve knowledge transfer, allowing models to achieve better
downstream performance over the latest data, and improve temporal
generalization when distribution gaps exist between training and evaluation
because of time. We believe our problem formulation, methods, and analysis will
inspire future studies towards continual pretraining of language models.
"
text,large language models transformers pretraining,19,2309.10931,0.7799159288406372,A Family of Pretrained Transformer Language Models for Russian,cs.CL,"  Transformer language models (LMs) are fundamental to NLP research
methodologies and applications in various languages. However, developing such
models specifically for the Russian language has received little attention.
This paper introduces a collection of 13 Russian Transformer LMs, which spans
encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder
(ruT5, FRED-T5) architectures. We provide a report on the model architecture
design and pretraining, and the results of evaluating their generalization
abilities on Russian language understanding and generation datasets and
benchmarks. By pretraining and releasing these specialized Transformer LMs, we
aim to broaden the scope of the NLP research directions and enable the
development of industrial solutions for the Russian language.
"
text,large language models transformers pretraining,20,2311.05640,0.7798824310302734,FinGPT: Large Generative Models for a Small Language,cs.CL,"  Large language models (LLMs) excel in many tasks in NLP and beyond, but most
open models have very limited coverage of smaller languages and LLM work tends
to focus on languages where nearly unlimited data is available for pretraining.
In this work, we study the challenges of creating LLMs for Finnish, a language
spoken by less than 0.1% of the world population. We compile an extensive
dataset of Finnish combining web crawls, news, social media and eBooks. We
pursue two approaches to pretrain models: 1) we train seven monolingual models
from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the
pretraining of the multilingual BLOOM model on a mix of its original training
data and Finnish, resulting in a 176 billion parameter model we call BLUUMI.
For model evaluation, we introduce FIN-bench, a version of BIG-bench with
Finnish tasks. We also assess other model qualities such as toxicity and bias.
Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.
"
