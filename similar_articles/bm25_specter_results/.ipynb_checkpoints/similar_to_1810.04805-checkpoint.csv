query_type,query_value,query_title,query_abstract,rank,doc_id,specter_score,title,categories,abstract
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",1,1908.04577,0.961064338684082,"StructBERT: Incorporating Language Structures into Pre-training for Deep
  Language Understanding",cs.CL,"  Recently, the pre-trained language model, BERT (and its robustly optimized
version RoBERTa), has attracted a lot of attention in natural language
understanding (NLU), and achieved state-of-the-art accuracy in various NLU
tasks, such as sentiment classification, natural language inference, semantic
textual similarity and question answering. Inspired by the linearization
exploration work of Elman [8], we extend BERT to a new model, StructBERT, by
incorporating language structures into pre-training. Specifically, we pre-train
StructBERT with two auxiliary tasks to make the most of the sequential order of
words and sentences, which leverage language structures at the word and
sentence levels, respectively. As a result, the new model is adapted to
different levels of language understanding required by downstream tasks. The
StructBERT with structural pre-training gives surprisingly good empirical
results on a variety of downstream tasks, including pushing the
state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published
models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on
SNLI to 91.7.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",2,1910.10683,0.9496079683303833,"Exploring the Limits of Transfer Learning with a Unified Text-to-Text
  Transformer",cs.LG cs.CL stat.ML,"  Transfer learning, where a model is first pre-trained on a data-rich task
before being fine-tuned on a downstream task, has emerged as a powerful
technique in natural language processing (NLP). The effectiveness of transfer
learning has given rise to a diversity of approaches, methodology, and
practice. In this paper, we explore the landscape of transfer learning
techniques for NLP by introducing a unified framework that converts all
text-based language problems into a text-to-text format. Our systematic study
compares pre-training objectives, architectures, unlabeled data sets, transfer
approaches, and other factors on dozens of language understanding tasks. By
combining the insights from our exploration with scale and our new ``Colossal
Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks
covering summarization, question answering, text classification, and more. To
facilitate future work on transfer learning for NLP, we release our data set,
pre-trained models, and code.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",3,2203.14507,0.9492318034172058,ANNA: Enhanced Language Representation for Question Answering,cs.CL,"  Pre-trained language models have brought significant improvements in
performance in a variety of natural language processing tasks. Most existing
models performing state-of-the-art results have shown their approaches in the
separate perspectives of data processing, pre-training tasks, neural network
modeling, or fine-tuning. In this paper, we demonstrate how the approaches
affect performance individually, and that the language model performs the best
results on a specific question answering task when those approaches are jointly
considered in pre-training models. In particular, we propose an extended
pre-training task, and a new neighbor-aware mechanism that attends neighboring
tokens more to capture the richness of context for pre-training language
modeling. Our best model achieves new state-of-the-art results of 95.7\% F1 and
90.6\% EM on SQuAD 1.1 and also outperforms existing pre-trained language
models such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmark.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",4,1903.12136,0.9489320516586304,Distilling Task-Specific Knowledge from BERT into Simple Neural Networks,cs.CL cs.LG,"  In the natural language processing literature, neural networks are becoming
increasingly deeper and complex. The recent poster child of this trend is the
deep language representation model, which includes BERT, ELMo, and GPT. These
developments have led to the conviction that previous-generation, shallower
neural networks for language understanding are obsolete. In this paper,
however, we demonstrate that rudimentary, lightweight neural networks can still
be made competitive without architecture changes, external training data, or
additional input features. We propose to distill knowledge from BERT, a
state-of-the-art language representation model, into a single-layer BiLSTM, as
well as its siamese counterpart for sentence-pair tasks. Across multiple
datasets in paraphrasing, natural language inference, and sentiment
classification, we achieve comparable results with ELMo, while using roughly
100 times fewer parameters and 15 times less inference time.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",5,1911.03090,0.9476167559623718,What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning,cs.CL,"  Pretrained transformer-based language models have achieved state of the art
across countless tasks in natural language processing. These models are highly
expressive, comprising at least a hundred million parameters and a dozen
layers. Recent evidence suggests that only a few of the final layers need to be
fine-tuned for high quality on downstream tasks. Naturally, a subsequent
research question is, ""how many of the last layers do we need to fine-tune?"" In
this paper, we precisely answer this question. We examine two recent pretrained
language models, BERT and RoBERTa, across standard tasks in textual entailment,
semantic similarity, sentiment analysis, and linguistic acceptability. We vary
the number of final layers that are fine-tuned, then study the resulting change
in task-specific effectiveness. We show that only a fourth of the final layers
need to be fine-tuned to achieve 90% of the original quality. Surprisingly, we
also find that fine-tuning all layers does not always help.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",6,2003.07000,0.946341872215271,"TRANS-BLSTM: Transformer with Bidirectional LSTM for Language
  Understanding",cs.CL cs.LG cs.SD eess.AS,"  Bidirectional Encoder Representations from Transformers (BERT) has recently
achieved state-of-the-art performance on a broad range of NLP tasks including
sentence classification, machine translation, and question answering. The BERT
model architecture is derived primarily from the transformer. Prior to the
transformer era, bidirectional Long Short-Term Memory (BLSTM) has been the
dominant modeling architecture for neural machine translation and question
answering. In this paper, we investigate how these two modeling techniques can
be combined to create a more powerful model architecture. We propose a new
architecture denoted as Transformer with BLSTM (TRANS-BLSTM) which has a BLSTM
layer integrated to each transformer block, leading to a joint modeling
framework for transformer and BLSTM. We show that TRANS-BLSTM models
consistently lead to improvements in accuracy compared to BERT baselines in
GLUE and SQuAD 1.1 experiments. Our TRANS-BLSTM model obtains an F1 score of
94.01% on the SQuAD 1.1 development dataset, which is comparable to the
state-of-the-art result.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",7,2004.14129,0.9442341327667236,How fine can fine-tuning be? Learning efficient language models,cs.CL cs.LG stat.ML,"  State-of-the-art performance on language understanding tasks is now achieved
with increasingly large networks; the current record holder has billions of
parameters. Given a language model pre-trained on massive unlabeled text
corpora, only very light supervised fine-tuning is needed to learn a task: the
number of fine-tuning steps is typically five orders of magnitude lower than
the total parameter count. Does this mean that fine-tuning only introduces
small differences from the pre-trained model in the parameter space? If so, can
one avoid storing and computing an entire model for each task? In this work, we
address these questions by using Bidirectional Encoder Representations from
Transformers (BERT) as an example. As expected, we find that the fine-tuned
models are close in parameter space to the pre-trained one, with the closeness
varying from layer to layer. We show that it suffices to fine-tune only the
most critical layers. Further, we find that there are surprisingly many good
solutions in the set of sparsified versions of the pre-trained model. As a
result, fine-tuning of huge language models can be achieved by simply setting a
certain number of entries in certain layers of the pre-trained parameters to
zero, saving both task-specific parameter storage and computational cost.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",8,2003.02912,0.9434798955917358,What the [MASK]? Making Sense of Language-Specific BERT Models,cs.CL,"  Recently, Natural Language Processing (NLP) has witnessed an impressive
progress in many areas, due to the advent of novel, pretrained contextual
representation models. In particular, Devlin et al. (2019) proposed a model,
called BERT (Bidirectional Encoder Representations from Transformers), which
enables researchers to obtain state-of-the art performance on numerous NLP
tasks by fine-tuning the representations on their data set and task, without
the need for developing and training highly-specific architectures. The authors
also released multilingual BERT (mBERT), a model trained on a corpus of 104
languages, which can serve as a universal language model. This model obtained
impressive results on a zero-shot cross-lingual natural inference task. Driven
by the potential of BERT models, the NLP community has started to investigate
and generate an abundant number of BERT models that are trained on a particular
language, and tested on a specific data domain and task. This allows us to
evaluate the true potential of mBERT as a universal language model, by
comparing it to the performance of these more specific models. This paper
presents the current state of the art in language-specific BERT models,
providing an overall picture with respect to different dimensions (i.e.
architectures, data domains, and tasks). Our aim is to provide an immediate and
straightforward overview of the commonalities and differences between
Language-Specific (language-specific) BERT models and mBERT. We also provide an
interactive and constantly updated website that can be used to explore the
information we have collected, at https://bertlang.unibocconi.it.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",9,2010.03760,0.9433146715164185,"Discriminatively-Tuned Generative Classifiers for Robust Natural
  Language Inference",cs.CL cs.LG,"  While discriminative neural network classifiers are generally preferred,
recent work has shown advantages of generative classifiers in term of data
efficiency and robustness. In this paper, we focus on natural language
inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and
empirically characterize its performance by comparing it to five baselines,
including discriminative models and large-scale pretrained language
representation models like BERT. We explore training objectives for
discriminative fine-tuning of our generative classifiers, showing improvements
over log loss fine-tuning from prior work . In particular, we find strong
results with a simple unbounded modification to log loss, which we call the
""infinilog loss"". Our experiments show that GenNLI outperforms both
discriminative and pretrained baselines across several challenging NLI
experimental settings, including small training sets, imbalanced label
distributions, and label noise.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",10,1901.11504,0.9431511163711548,Multi-Task Deep Neural Networks for Natural Language Understanding,cs.CL,"  In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for
learning representations across multiple natural language understanding (NLU)
tasks. MT-DNN not only leverages large amounts of cross-task data, but also
benefits from a regularization effect that leads to more general
representations in order to adapt to new tasks and domains. MT-DNN extends the
model proposed in Liu et al. (2015) by incorporating a pre-trained
bidirectional transformer language model, known as BERT (Devlin et al., 2018).
MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI,
SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7%
(2.2% absolute improvement). We also demonstrate using the SNLI and SciTail
datasets that the representations learned by MT-DNN allow domain adaptation
with substantially fewer in-domain labels than the pre-trained BERT
representations. The code and pre-trained models are publicly available at
https://github.com/namisan/mt-dnn.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",11,1902.02671,0.942829966545105,"BERT and PALs: Projected Attention Layers for Efficient Adaptation in
  Multi-Task Learning",cs.LG cs.CL stat.ML,"  Multi-task learning shares information between related tasks, sometimes
reducing the number of parameters required. State-of-the-art results across
multiple natural language understanding tasks in the GLUE benchmark have
previously used transfer from a single large task: unsupervised pre-training
with BERT, where a separate BERT model was fine-tuned for each task. We explore
multi-task approaches that share a single BERT model with a small number of
additional task-specific parameters. Using new adaptation modules, PALs or
`projected attention layers', we match the performance of separately fine-tuned
models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain
state-of-the-art results on the Recognizing Textual Entailment dataset.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",12,1910.13461,0.9415028691291809,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
  Generation, Translation, and Comprehension",cs.CL cs.LG stat.ML,"  We present BART, a denoising autoencoder for pretraining sequence-to-sequence
models. BART is trained by (1) corrupting text with an arbitrary noising
function, and (2) learning a model to reconstruct the original text. It uses a
standard Tranformer-based neural machine translation architecture which,
despite its simplicity, can be seen as generalizing BERT (due to the
bidirectional encoder), GPT (with the left-to-right decoder), and many other
more recent pretraining schemes. We evaluate a number of noising approaches,
finding the best performance by both randomly shuffling the order of the
original sentences and using a novel in-filling scheme, where spans of text are
replaced with a single mask token. BART is particularly effective when fine
tuned for text generation but also works well for comprehension tasks. It
matches the performance of RoBERTa with comparable training resources on GLUE
and SQuAD, achieves new state-of-the-art results on a range of abstractive
dialogue, question answering, and summarization tasks, with gains of up to 6
ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system
for machine translation, with only target language pretraining. We also report
ablation experiments that replicate other pretraining schemes within the BART
framework, to better measure which factors most influence end-task performance.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",13,1909.11556,0.9408650398254395,Reducing Transformer Depth on Demand with Structured Dropout,cs.LG cs.CL stat.ML,"  Overparameterized transformer networks have obtained state of the art results
in various natural language processing tasks, such as machine translation,
language modeling, and question answering. These models contain hundreds of
millions of parameters, necessitating a large amount of computation and making
them prone to overfitting. In this work, we explore LayerDrop, a form of
structured dropout, which has a regularization effect during training and
allows for efficient pruning at inference time. In particular, we show that it
is possible to select sub-networks of any depth from one large network without
having to finetune them and with limited impact on performance. We demonstrate
the effectiveness of our approach by improving the state of the art on machine
translation, language modeling, summarization, question answering, and language
understanding benchmarks. Moreover, we show that our approach leads to small
BERT-like models of higher quality compared to training from scratch or using
distillation.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",14,1912.07076,0.9397298693656921,Multilingual is not enough: BERT for Finnish,cs.CL,"  Deep learning-based language models pretrained on large unannotated text
corpora have been demonstrated to allow efficient transfer learning for natural
language processing, with recent approaches such as the transformer-based BERT
model advancing the state of the art across a variety of tasks. While most work
on these models has focused on high-resource languages, in particular English,
a number of recent efforts have introduced multilingual models that can be
fine-tuned to address tasks in a large number of different languages. However,
we still lack a thorough understanding of the capabilities of these models, in
particular for lower-resourced languages. In this paper, we focus on Finnish
and thoroughly evaluate the multilingual BERT model on a range of tasks,
comparing it with a new Finnish BERT model trained from scratch. The new
language-specific model is shown to systematically and clearly outperform the
multilingual. While the multilingual model largely fails to reach the
performance of previously proposed methods, the custom Finnish BERT model
establishes new state-of-the-art results on all corpora for all reference
tasks: part-of-speech tagging, named entity recognition, and dependency
parsing. We release the model and all related resources created for this study
with open licenses at https://turkunlp.org/finbert .
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",15,2205.11726,0.9393975734710693,On the Role of Bidirectionality in Language Model Pre-Training,cs.CL cs.AI cs.LG,"  Prior work on language model pre-training has explored different
architectures and learning objectives, but differences in data, hyperparameters
and evaluation make a principled comparison difficult. In this work, we focus
on bidirectionality as a key factor that differentiates existing approaches,
and present a comprehensive study of its role in next token prediction, text
infilling, zero-shot priming and fine-tuning. We propose a new framework that
generalizes prior approaches, including fully unidirectional models like GPT,
fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.
Our framework distinguishes between two notions of bidirectionality
(bidirectional context and bidirectional attention) and allows us to control
each of them separately. We find that the optimal configuration is largely
application-dependent (e.g., bidirectional attention is beneficial for
fine-tuning and infilling, but harmful for next token prediction and zero-shot
priming). We train models with up to 6.7B parameters, and find differences to
remain consistent at scale. While prior work on scaling has focused on
left-to-right autoregressive models, our results suggest that this approach
comes with some trade-offs, and it might be worthwhile to develop very large
bidirectional models.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",16,1905.07588,0.939376950263977,BERTSel: Answer Selection with Pre-trained Models,cs.CL,"  Recently, pre-trained models have been the dominant paradigm in natural
language processing. They achieved remarkable state-of-the-art performance
across a wide range of related tasks, such as textual entailment, natural
language inference, question answering, etc. BERT, proposed by Devlin et.al.,
has achieved a better marked result in GLUE leaderboard with a deep transformer
architecture. Despite its soaring popularity, however, BERT has not yet been
applied to answer selection. This task is different from others with a few
nuances: first, modeling the relevance and correctness of candidates matters
compared to semantic relatedness and syntactic structure; second, the length of
an answer may be different from other candidates and questions. In this paper.
we are the first to explore the performance of fine-tuning BERT for answer
selection. We achieved STOA results across five popular datasets, demonstrating
the success of pre-trained models in this task.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",17,2111.04909,0.939344584941864,FPM: A Collection of Large-scale Foundation Pre-trained Language Models,cs.CL cs.AI,"  Large-scale Transformer models have significantly promoted the recent
development of natural language processing applications. However, little effort
has been made to unify the effective models. In this paper, driven by providing
a new set of baseline models in the future, we adopt various novel transformer
architectures and launch a model set with the help of recent mainstream
technologies. We focus the discussions on optimizing the depth of the networks
based on the existing powerful encode-decoder structures. We show that by
properly avoiding training defects such as non-convergence and degradation,
scaling up off-the-shelf transformer architectures consistently delivers better
performance. To stimulate future research on large-scale language model
pretraining, we present extensive results and detailed discussions on network
performance improvements with respect to the network depth and confirm the
existence of the optimal number of layers under specific tasks. To the best of
our knowledge, we provide the largest Chinese generative model and the largest
Chinese encoding model. The BERT language models we trained on English datasets
deliver a 14.45% higher F1 score than the Turing-NLR.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",18,2502.19587,0.9390590190887451,NeoBERT: A Next-Generation BERT,cs.CL cs.AI,"Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption."
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",19,1904.09286,0.9390095472335815,"Unifying Question Answering, Text Classification, and Regression via
  Span Extraction",cs.CL,"  Even as pre-trained language encoders such as BERT are shared across many
tasks, the output layers of question answering, text classification, and
regression models are significantly different. Span decoders are frequently
used for question answering, fixed-class, classification layers for text
classification, and similarity-scoring layers for regression tasks, We show
that this distinction is not necessary and that all three can be unified as
span extraction. A unified, span-extraction approach leads to superior or
comparable performance in supplementary supervised pre-trained, low-data, and
multi-task learning experiments on several question answering, text
classification, and regression benchmarks.
"
id,1810.04805,"BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding","  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
",20,1909.11942,0.938996434211731,"ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations",cs.CL cs.AI,"  Increasing model size when pretraining natural language representations often
results in improved performance on downstream tasks. However, at some point
further model increases become harder due to GPU/TPU memory limitations and
longer training times. To address these problems, we present two
parameter-reduction techniques to lower memory consumption and increase the
training speed of BERT. Comprehensive empirical evidence shows that our
proposed methods lead to models that scale much better compared to the original
BERT. We also use a self-supervised loss that focuses on modeling
inter-sentence coherence, and show it consistently helps downstream tasks with
multi-sentence inputs. As a result, our best model establishes new
state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having
fewer parameters compared to BERT-large. The code and the pretrained models are
available at https://github.com/google-research/ALBERT.
"
