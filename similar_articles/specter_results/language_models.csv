query_id,query_text,rank,doc_id,specter_score,title,categories,abstract
language_models,large language models transformers pretraining,1,2410.24159,0.80035543,GPT or BERT: why not both?,cs.CL,"  We present a simple way to merge masked language modeling with causal
language modeling. This hybrid training objective results in a model that
combines the strengths of both modeling paradigms within a single transformer
stack: GPT-BERT can be transparently used like any standard causal or masked
language model. We test the pretraining process that enables this flexible
behavior on the BabyLM Challenge 2024. The results show that the hybrid
pretraining outperforms masked-only or causal-only models. We openly release
the models, training corpora and code.
"
language_models,large language models transformers pretraining,2,1910.03771,0.80005115,HuggingFace's Transformers: State-of-the-art Natural Language Processing,cs.CL,"  Recent progress in natural language processing has been driven by advances in
both model architecture and model pretraining. Transformer architectures have
facilitated building higher-capacity models and pretraining has made it
possible to effectively utilize this capacity for a wide variety of tasks.
\textit{Transformers} is an open-source library with the goal of opening up
these advances to the wider machine learning community. The library consists of
carefully engineered state-of-the art Transformer architectures under a unified
API. Backing this library is a curated collection of pretrained models made by
and available for the community. \textit{Transformers} is designed to be
extensible by researchers, simple for practitioners, and fast and robust in
industrial deployments. The library is available at
\url{https://github.com/huggingface/transformers}.
"
language_models,large language models transformers pretraining,3,2305.14857,0.7990422,"BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual
  Transfer",cs.CL,"  Despite remarkable advancements in few-shot generalization in natural
language processing, most models are developed and evaluated primarily in
English. To facilitate research on few-shot cross-lingual transfer, we
introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across
54 languages in a sequence-to-sequence format and provides a fixed set of
few-shot examples and instructions. BUFFET is designed to establish a rigorous
and equitable evaluation framework for few-shot cross-lingual transfer across a
broad range of tasks and languages. Using BUFFET, we perform thorough
evaluations of state-of-the-art multilingual large language models with
different transfer methods, namely in-context learning and fine-tuning. Our
findings reveal significant room for improvement in few-shot in-context
cross-lingual transfer. In particular, ChatGPT with in-context learning often
performs worse than much smaller mT5-base models fine-tuned on English task
data and few-shot in-language examples. Our analysis suggests various avenues
for future research in few-shot cross-lingual transfer, such as improved
pretraining, understanding, and future evaluations.
"
language_models,large language models transformers pretraining,4,2402.19204,0.79385483,"PeLLE: Encoder-based language models for Brazilian Portuguese based on
  open data",cs.CL,"  In this paper we present PeLLE, a family of large language models based on
the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open
data from the Carolina corpus. Aiming at reproducible results, we describe
details of the pretraining of the models. We also evaluate PeLLE models against
a set of existing multilingual and PT-BR refined pretrained Transformer-based
LLM encoders, contrasting performance of large versus smaller-but-curated
pretrained models in several downstream tasks. We conclude that several tasks
perform better with larger models, but some tasks benefit from
smaller-but-curated data in its pretraining.
"
language_models,large language models transformers pretraining,5,2308.16336,0.7935762,"ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language
  Understanding",cs.CL cs.LG,"  We present ToddlerBERTa, a BabyBERTa-like language model, exploring its
capabilities through five different models with varied hyperparameters.
Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the
BabyLM challenge, we find that smaller models can excel in specific tasks,
while larger models perform well with substantial data. Despite training on a
smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling
the state-of-the-art RoBERTa-base. The model showcases robust language
understanding, even with single-sentence pretraining, and competes with
baselines that leverage broader contextual information. Our work provides
insights into hyperparameter choices, and data utilization, contributing to the
advancement of language models.
"
language_models,large language models transformers pretraining,6,2011.04784,0.79340124,EstBERT: A Pretrained Language-Specific BERT for Estonian,cs.CL,"  This paper presents EstBERT, a large pretrained transformer-based
language-specific BERT model for Estonian. Recent work has evaluated
multilingual BERT models on Estonian tasks and found them to outperform the
baselines. Still, based on existing studies on other languages, a
language-specific BERT model is expected to improve over the multilingual ones.
We first describe the EstBERT pretraining process and then present the results
of the models based on finetuned EstBERT for multiple NLP tasks, including POS
and morphological tagging, named entity recognition and text classification.
The evaluation results show that the models based on EstBERT outperform
multilingual BERT models on five tasks out of six, providing further evidence
towards a view that training language-specific BERT models are still useful,
even when multilingual models are available.
"
language_models,large language models transformers pretraining,7,2311.02265,0.7919848,Not all layers are equally as important: Every Layer Counts BERT,cs.CL,"  This paper introduces a novel modification of the transformer architecture,
tailored for the data-efficient pretraining of language models. This aspect is
evaluated by participating in the BabyLM challenge, where our solution won both
the strict and strict-small tracks. Our approach allows each transformer layer
to select which outputs of previous layers to process. The empirical results
verify the potential of this simple modification and show that not all layers
are equally as important.
"
language_models,large language models transformers pretraining,8,2412.04092,0.79076755,GEITje 7B Ultra: A Conversational Model for Dutch,cs.CL,"  Language models have rapidly evolved, predominantly focusing on English while
often neglecting extensive pretraining in other languages. This approach has
required initiatives to adapt powerful, English-centric models to other
linguistic contexts through finetuning. For Dutch, such a recent endeavour is
``GEITje'' a model originally derived from the English-based Mistral 7B.
Building on this fundamental work, the current research extends the
capabilities of GEITje by supervised finetuning on newly created high-quality
synthetic conversational datasets, along with an additional preference
alignment procedure on a synthetic feedback dataset. Both the developed models
and the created datasets are openly available.
"
language_models,large language models transformers pretraining,9,2102.12982,0.7891,"A Primer on Contrastive Pretraining in Language Processing: Methods,
  Lessons Learned and Perspectives",cs.CL cs.AI cs.CV,"  Modern natural language processing (NLP) methods employ self-supervised
pretraining objectives such as masked language modeling to boost the
performance of various application tasks. These pretraining methods are
frequently extended with recurrence, adversarial or linguistic property
masking, and more recently with contrastive learning objectives. Contrastive
self-supervised training objectives enabled recent successes in image
representation pretraining by learning to contrast input-input pairs of
augmented images as either similar or dissimilar. However, in NLP, automated
creation of text input augmentations is still very challenging because a single
token can invert the meaning of a sentence. For this reason, some contrastive
NLP pretraining methods contrast over input-label pairs, rather than over
input-input pairs, using methods from Metric Learning and Energy Based Models.
In this survey, we summarize recent self-supervised and supervised contrastive
NLP pretraining methods and describe where they are used to improve language
modeling, few or zero-shot learning, pretraining data-efficiency and specific
NLP end-tasks. We introduce key contrastive learning concepts with lessons
learned from prior research and structure works by applications and cross-field
relations. Finally, we point to open challenges and future directions for
contrastive NLP to encourage bringing contrastive NLP pretraining closer to
recent successes in image representation pretraining.
"
language_models,large language models transformers pretraining,10,2006.13979,0.7879652,"Unsupervised Cross-lingual Representation Learning for Speech
  Recognition",cs.CL cs.LG cs.SD eess.AS,"  This paper presents XLSR which learns cross-lingual speech representations by
pretraining a single model from the raw waveform of speech in multiple
languages. We build on wav2vec 2.0 which is trained by solving a contrastive
task over masked latent speech representations and jointly learns a
quantization of the latents shared across languages. The resulting model is
fine-tuned on labeled data and experiments show that cross-lingual pretraining
significantly outperforms monolingual pretraining. On the CommonVoice
benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared
to the best known results. On BABEL, our approach improves word error rate by
16% relative compared to a comparable system. Our approach enables a single
multilingual speech recognition model which is competitive to strong individual
models. Analysis shows that the latent discrete speech representations are
shared across languages with increased sharing for related languages. We hope
to catalyze research in low-resource speech understanding by releasing XLSR-53,
a large model pretrained in 53 languages.
"
