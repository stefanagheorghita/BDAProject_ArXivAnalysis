query_id,query_text,rank,doc_id,specter_score,title,categories,abstract
optimization,convex optimization gradient descent convergence,1,1810.12273,0.814426,"Kalman Gradient Descent: Adaptive Variance Reduction in Stochastic
  Optimization",stat.ML cs.LG math.OC,"  We introduce Kalman Gradient Descent, a stochastic optimization algorithm
that uses Kalman filtering to adaptively reduce gradient variance in stochastic
gradient descent by filtering the gradient estimates. We present both a
theoretical analysis of convergence in a non-convex setting and experimental
results which demonstrate improved performance on a variety of machine learning
areas including neural networks and black box variational inference. We also
present a distributed version of our algorithm that enables large-dimensional
optimization, and we extend our algorithm to SGD with momentum and RMSProp.
"
optimization,convex optimization gradient descent convergence,2,1708.00555,0.8130057,Mini-batch stochastic gradient descent with dynamic sample sizes,math.OC,"  We focus on solving constrained convex optimization problems using mini-batch
stochastic gradient descent. Dynamic sample size rules are presented which
ensure a descent direction with high probability. Empirical results from two
applications show superior convergence compared to fixed sample
implementations.
"
optimization,convex optimization gradient descent convergence,3,2106.08020,0.81086445,"A note on the optimal convergence rate of descent methods with fixed
  step sizes for smooth strongly convex functions",math.OC,"  Based on a result by Taylor, Hendrickx, and Glineur (J. Optim. Theory Appl.,
178(2):455--476, 2018) on the attainable convergence rate of gradient descent
for smooth and strongly convex functions in terms of function values, an
elementary convergence analysis for general descent methods with fixed step
sizes is presented. It covers general variable metric methods, gradient related
search directions under angle and scaling conditions, as well as inexact
gradient methods. In all cases, optimal rates are obtained.
"
optimization,convex optimization gradient descent convergence,4,1807.06574,0.8099245,"Jensen: An Easily-Extensible C++ Toolkit for Production-Level Machine
  Learning and Convex Optimization",cs.LG math.OC stat.ML,"  This paper introduces Jensen, an easily extensible and scalable toolkit for
production-level machine learning and convex optimization. Jensen implements a
framework of convex (or loss) functions, convex optimization algorithms
(including Gradient Descent, L-BFGS, Stochastic Gradient Descent, Conjugate
Gradient, etc.), and a family of machine learning classifiers and regressors
(Logistic Regression, SVMs, Least Square Regression, etc.). This framework
makes it possible to deploy and train models with a few lines of code, and also
extend and build upon this by integrating new loss functions and optimization
algorithms.
"
optimization,convex optimization gradient descent convergence,5,2412.06070,0.809567,Stochastic Gradient Descent Revisited,math.OC math.PR stat.ML,"  Stochastic gradient descent (SGD) has been a go-to algorithm for nonconvex
stochastic optimization problems arising in machine learning. Its theory
however often requires a strong framework to guarantee convergence properties.
We hereby present a full scope convergence study of biased nonconvex SGD,
including weak convergence, function-value convergence and global convergence,
and also provide subsequent convergence rates and complexities, all under
relatively mild conditions in comparison with literature.
"
optimization,convex optimization gradient descent convergence,6,2110.01858,0.80633616,"KKT Conditions, First-Order and Second-Order Optimization, and
  Distributed Optimization: Tutorial and Survey",math.OC cs.DC cs.LG cs.NA math.NA,"  This is a tutorial and survey paper on Karush-Kuhn-Tucker (KKT) conditions,
first-order and second-order numerical optimization, and distributed
optimization. After a brief review of history of optimization, we start with
some preliminaries on properties of sets, norms, functions, and concepts of
optimization. Then, we introduce the optimization problem, standard
optimization problems (including linear programming, quadratic programming, and
semidefinite programming), and convex problems. We also introduce some
techniques such as eliminating inequality, equality, and set constraints,
adding slack variables, and epigraph form. We introduce Lagrangian function,
dual variables, KKT conditions (including primal feasibility, dual feasibility,
weak and strong duality, complementary slackness, and stationarity condition),
and solving optimization by method of Lagrange multipliers. Then, we cover
first-order optimization including gradient descent, line-search, convergence
of gradient methods, momentum, steepest descent, and backpropagation. Other
first-order methods are explained, such as accelerated gradient method,
stochastic gradient descent, mini-batch gradient descent, stochastic average
gradient, stochastic variance reduced gradient, AdaGrad, RMSProp, and Adam
optimizer, proximal methods (including proximal mapping, proximal point
algorithm, and proximal gradient method), and constrained gradient methods
(including projected gradient method, projection onto convex sets, and
Frank-Wolfe method). We also cover non-smooth and $\ell_1$ optimization methods
including lasso regularization, convex conjugate, Huber function,
soft-thresholding, coordinate descent, and subgradient methods. Then, we
explain second-order methods including Newton's method for unconstrained,
equality constrained, and inequality constrained problems....
"
optimization,convex optimization gradient descent convergence,7,1911.08380,0.803966,"Adaptive Gradient Descent for Convex and Non-Convex Stochastic
  Optimization",math.OC,"  In this paper we propose several adaptive gradient methods for stochastic
optimization. Unlike AdaGrad-type of methods, our algorithms are based on
Armijo-type line search and they simultaneously adapt to the unknown Lipschitz
constant of the gradient and variance of the stochastic approximation for the
gradient. We consider an accelerated and non-accelerated gradient descent for
convex problems and gradient descent for non-convex problems. In the
experiments we demonstrate superiority of our methods to existing adaptive
methods, e.g. AdaGrad and Adam.
"
optimization,convex optimization gradient descent convergence,8,2010.05109,0.8024344,AEGD: Adaptive Gradient Descent with Energy,math.OC cs.LG cs.NA math.NA stat.ML,"  We propose AEGD, a new algorithm for first-order gradient-based optimization
of non-convex objective functions, based on a dynamically updated energy
variable. The method is shown to be unconditionally energy stable, irrespective
of the step size. We prove energy-dependent convergence rates of AEGD for both
non-convex and convex objectives, which for a suitably small step size recovers
desired convergence rates for the batch gradient descent. We also provide an
energy-dependent bound on the stationary convergence of AEGD in the stochastic
non-convex setting. The method is straightforward to implement and requires
little tuning of hyper-parameters. Experimental results demonstrate that AEGD
works well for a large variety of optimization problems: it is robust with
respect to initial data, capable of making rapid initial progress. The
stochastic AEGD shows comparable and often better generalization performance
than SGD with momentum for deep neural networks.
"
optimization,convex optimization gradient descent convergence,9,2301.11235,0.8001175,Handbook of Convergence Theorems for (Stochastic) Gradient Methods,math.OC,"  This is a handbook of simple proofs of the convergence of gradient and
stochastic gradient descent type methods. We consider functions that are
Lipschitz, smooth, convex, strongly convex, and/or Polyak-{\L}ojasiewicz
functions. Our focus is on ``good proofs'' that are also simple. Each section
can be consulted separately. We start with proofs of gradient descent, then on
stochastic variants, including minibatching and momentum. Then move on to
nonsmooth problems with the subgradient method, the proximal gradient descent
and their stochastic variants. Our focus is on global convergence rates and
complexity rates. Some slightly less common proofs found here include that of
SGD (Stochastic gradient descent) with a proximal step, with momentum, and with
mini-batching without replacement.
"
optimization,convex optimization gradient descent convergence,10,1506.02186,0.79297876,A Universal Catalyst for First-Order Optimization,math.OC,"  We introduce a generic scheme for accelerating first-order optimization
methods in the sense of Nesterov, which builds upon a new analysis of the
accelerated proximal point algorithm. Our approach consists of minimizing a
convex objective by approximately solving a sequence of well-chosen auxiliary
problems, leading to faster convergence. This strategy applies to a large class
of algorithms, including gradient descent, block coordinate descent, SAG, SAGA,
SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods,
we provide acceleration and explicit support for non-strongly convex
objectives. In addition to theoretical speed-up, we also show that acceleration
is useful in practice, especially for ill-conditioned problems where we measure
significant improvements.
"
